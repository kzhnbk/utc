{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Начиная с вашего запроса, вот подробное руководство по использованию XGBoost, CatBoost, LightGBM, PyTorch Lightning и Transformers, включая методы, примеры с NumPy и PyTorch, объяснения и шаги по обучению на вашем собственном наборе данных.\n",
        "\n",
        "## Руководство по использованию библиотек машинного обучения\n",
        "\n",
        "Это руководство охватывает пять мощных библиотек, широко используемых в машинном обучении и глубоком обучении.\n",
        "\n",
        "### 1\\. XGBoost (Extreme Gradient Boosting)\n",
        "\n",
        "XGBoost - это оптимизированная распределенная библиотека градиентного бустинга, предназначенная для высокой производительности, гибкости и переносимости. Она реализует алгоритмы машинного обучения под общим фреймворком градиентного бустинга.\n",
        "\n",
        "**Что это делает:**\n",
        "XGBoost специализируется на задачах табличных данных, таких как классификация, регрессия и ранжирование. Он очень эффективен, масштабируем и часто выигрывает соревнования по машинному обучению благодаря своей способности обрабатывать различные типы данных, обрабатывать пропущенные значения и выполнять регуляризацию для предотвращения переобучения.\n",
        "\n",
        "**Установка:**\n",
        "`pip install xgboost`\n",
        "\n",
        "**Основные методы и атрибуты:**\n",
        "\n",
        "  * `XGBClassifier`: Для задач классификации.\n",
        "  * `XGBRegressor`: Для задач регрессии.\n",
        "  * `fit(X, y)`: Обучает модель.\n",
        "  * `predict(X)`: Делает предсказания.\n",
        "  * `predict_proba(X)`: Возвращает вероятности классов для классификации.\n",
        "  * `feature_importances_`: Важность признаков после обучения.\n",
        "  * `set_params(**params)`: Устанавливает параметры для модели.\n",
        "\n",
        "**Примеры использования с NumPy и PyTorch:**\n",
        "\n",
        "**Пример с NumPy (Классификация):**"
      ],
      "metadata": {
        "id": "FLVKtixb8Wa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Генерация синтетических данных NumPy\n",
        "X_np = np.random.rand(1000, 10) # 1000 образцов, 10 признаков\n",
        "y_np = np.random.randint(0, 2, 1000) # Бинарная метка (0 или 1)\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
        "\n",
        "# Инициализация и обучение XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
        "xgb_model.fit(X_train_np, y_train_np)\n",
        "\n",
        "# Предсказания\n",
        "y_pred_np = xgb_model.predict(X_test_np)\n",
        "accuracy_np = accuracy_score(y_test_np, y_pred_np)\n",
        "print(f\"XGBoost (NumPy) Accuracy: {accuracy_np:.4f}\")\n",
        "\n",
        "# Важность признаков\n",
        "print(\"XGBoost (NumPy) Feature Importances:\", xgb_model.feature_importances_)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "TWPQdf_c8Wa_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Пример с PyTorch (классификация, используя .numpy() для преобразования):**\n",
        "\n",
        "XGBoost не принимает напрямую тензоры PyTorch. Вам нужно преобразовать их в массивы NumPy."
      ],
      "metadata": {
        "id": "OL8emPMi8WbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Генерация синтетических данных PyTorch\n",
        "X_torch = torch.rand(1000, 10)\n",
        "y_torch = torch.randint(0, 2, (1000,))\n",
        "\n",
        "# Преобразование в NumPy для XGBoost\n",
        "X_np_from_torch = X_torch.numpy()\n",
        "y_np_from_torch = y_torch.numpy()\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки (теперь в NumPy)\n",
        "X_train_np_t, X_test_np_t, y_train_np_t, y_test_np_t = train_test_split(X_np_from_torch, y_np_from_torch, test_size=0.2, random_state=42)\n",
        "\n",
        "# Инициализация и обучение XGBoost Classifier\n",
        "xgb_model_torch = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
        "xgb_model_torch.fit(X_train_np_t, y_train_np_t)\n",
        "\n",
        "# Предсказания\n",
        "y_pred_np_t = xgb_model_torch.predict(X_test_np_t)\n",
        "accuracy_np_t = accuracy_score(y_test_np_t, y_pred_np_t)\n",
        "print(f\"XGBoost (PyTorch -> NumPy) Accuracy: {accuracy_np_t:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "rc53Kz_H8WbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2\\. CatBoost\n",
        "\n",
        "CatBoost - это алгоритм градиентного бустинга на деревьях решений, разработанный Яндекс. Он отличается специализированной обработкой категориальных признаков и симметричными деревьями.\n",
        "\n",
        "**Что это делает:**\n",
        "CatBoost предназначен для работы с категориальными данными без предварительной обработки, такой как one-hot encoding. Он использует специальную схему кодирования категориальных признаков во время обучения, что помогает снизить переобучение. Он также известен своей высокой точностью и скоростью.\n",
        "\n",
        "**Установка:**\n",
        "`pip install catboost`\n",
        "\n",
        "**Основные методы и атрибуты:**\n",
        "\n",
        "  * `CatBoostClassifier`: Для задач классификации.\n",
        "  * `CatBoostRegressor`: Для задач регрессии.\n",
        "  * `fit(X, y, cat_features)`: Обучает модель. `cat_features` - необязательный список индексов категориальных признаков.\n",
        "  * `predict(X)`: Делает предсказания.\n",
        "  * `predict_proba(X)`: Возвращает вероятности классов для классификации.\n",
        "  * `get_feature_importance()`: Важность признаков после обучения.\n",
        "\n",
        "**Примеры использования с NumPy и PyTorch:**\n",
        "\n",
        "**Пример с NumPy (Классификация):**"
      ],
      "metadata": {
        "id": "kL0dFt8q8WbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Генерация синтетических данных NumPy с категориальными признаками\n",
        "data_np = {\n",
        "    'feature1': np.random.rand(1000),\n",
        "    'feature2': np.random.randint(0, 3, 1000).astype(str), # Категориальный признак\n",
        "    'feature3': np.random.rand(1000),\n",
        "    'target': np.random.randint(0, 2, 1000)\n",
        "}\n",
        "df_np = pd.DataFrame(data_np)\n",
        "\n",
        "X_np = df_np[['feature1', 'feature2', 'feature3']]\n",
        "y_np = df_np['target']\n",
        "\n",
        "# Определяем индексы категориальных признаков\n",
        "categorical_features_indices_np = [X_np.columns.get_loc('feature2')]\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
        "\n",
        "# Инициализация и обучение CatBoost Classifier\n",
        "cat_model = CatBoostClassifier(iterations=100,  # Количество деревьев\n",
        "                                 learning_rate=0.1,\n",
        "                                 depth=6,\n",
        "                                 loss_function='Logloss',\n",
        "                                 eval_metric='Accuracy',\n",
        "                                 random_seed=42,\n",
        "                                 verbose=False) # Отключение подробного вывода\n",
        "cat_model.fit(X_train_np, y_train_np, cat_features=categorical_features_indices_np)\n",
        "\n",
        "# Предсказания\n",
        "y_pred_cat_np = cat_model.predict(X_test_np)\n",
        "accuracy_cat_np = accuracy_score(y_test_np, y_pred_cat_np)\n",
        "print(f\"CatBoost (NumPy) Accuracy: {accuracy_cat_np:.4f}\")\n",
        "\n",
        "# Важность признаков\n",
        "print(\"CatBoost (NumPy) Feature Importances:\", cat_model.get_feature_importance())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "R3I7GyXm8WbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Пример с PyTorch (Использование .numpy() для преобразования):**\n",
        "\n",
        "Как и XGBoost, CatBoost ожидает массивы NumPy или Pandas DataFrame."
      ],
      "metadata": {
        "id": "nxubKgb_8WbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Генерация синтетических данных PyTorch с категориальными признаками\n",
        "X_torch = torch.rand(1000, 2) # Числовые признаки\n",
        "# Категориальный признак, преобразованный в строки для CatBoost\n",
        "cat_torch = torch.randint(0, 3, (1000,)).numpy().astype(str)\n",
        "y_torch = torch.randint(0, 2, (1000,))\n",
        "\n",
        "# Преобразование в Pandas DataFrame для CatBoost\n",
        "df_torch = pd.DataFrame(X_torch.numpy(), columns=['feature1', 'feature3'])\n",
        "df_torch['feature2'] = cat_torch\n",
        "y_np_from_torch = y_torch.numpy()\n",
        "\n",
        "categorical_features_indices_torch = [df_torch.columns.get_loc('feature2')]\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки (теперь в Pandas)\n",
        "X_train_torch_df, X_test_torch_df, y_train_torch_np, y_test_torch_np = train_test_split(df_torch, y_np_from_torch, test_size=0.2, random_state=42)\n",
        "\n",
        "# Инициализация и обучение CatBoost Classifier\n",
        "cat_model_torch = CatBoostClassifier(iterations=100,\n",
        "                                       learning_rate=0.1,\n",
        "                                       depth=6,\n",
        "                                       loss_function='Logloss',\n",
        "                                       eval_metric='Accuracy',\n",
        "                                       random_seed=42,\n",
        "                                       verbose=False)\n",
        "cat_model_torch.fit(X_train_torch_df, y_train_torch_np, cat_features=categorical_features_indices_torch)\n",
        "\n",
        "# Предсказания\n",
        "y_pred_cat_torch = cat_model_torch.predict(X_test_torch_df)\n",
        "accuracy_cat_torch = accuracy_score(y_test_torch_np, y_pred_cat_torch)\n",
        "print(f\"CatBoost (PyTorch -> Pandas) Accuracy: {accuracy_cat_torch:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "loSlluaj8WbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3\\. LightGBM (Light Gradient Boosting Machine)\n",
        "\n",
        "LightGBM - это фреймворк градиентного бустинга, разработанный Microsoft, который использует древовидные алгоритмы обучения. Он разработан для высокой скорости и распределенного обучения.\n",
        "\n",
        "**Что это делает:**\n",
        "LightGBM оптимизирован для больших наборов данных, используя технику на основе гистограммы, которая группирует непрерывные значения признаков в дискретные интервалы (бины), что значительно ускоряет обучение. Он также использует алгоритм роста дерева на основе листьев (leaf-wise), в отличие от алгоритма роста дерева по уровню (level-wise), используемого в других реализациях. Это часто приводит к более быстрым и точным моделям.\n",
        "\n",
        "**Установка:**\n",
        "`pip install lightgbm`\n",
        "\n",
        "**Основные методы и атрибуты:**\n",
        "\n",
        "  * `LGBMClassifier`: Для задач классификации.\n",
        "  * `LGBMRegressor`: Для задач регрессии.\n",
        "  * `fit(X, y)`: Обучает модель.\n",
        "  * `predict(X)`: Делает предсказания.\n",
        "  * `predict_proba(X)`: Возвращает вероятности классов для классификации.\n",
        "  * `feature_importances_`: Важность признаков после обучения.\n",
        "\n",
        "**Примеры использования с NumPy и PyTorch:**\n",
        "\n",
        "**Пример с NumPy (Классификация):**"
      ],
      "metadata": {
        "id": "-z3xgafZ8WbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Генерация синтетических данных NumPy\n",
        "X_np = np.random.rand(1000, 10)\n",
        "y_np = np.random.randint(0, 2, 1000)\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
        "\n",
        "# Инициализация и обучение LightGBM Classifier\n",
        "lgb_model = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', random_state=42)\n",
        "lgb_model.fit(X_train_np, y_train_np)\n",
        "\n",
        "# Предсказания\n",
        "y_pred_lgb_np = lgb_model.predict(X_test_np)\n",
        "accuracy_lgb_np = accuracy_score(y_test_np, y_pred_lgb_np)\n",
        "print(f\"LightGBM (NumPy) Accuracy: {accuracy_lgb_np:.4f}\")\n",
        "\n",
        "# Важность признаков\n",
        "print(\"LightGBM (NumPy) Feature Importances:\", lgb_model.feature_importances_)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "nLZSHOcF8WbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Пример с PyTorch (Использование .numpy() для преобразования):**\n",
        "\n",
        "LightGBM также ожидает массивы NumPy или Pandas DataFrame."
      ],
      "metadata": {
        "id": "LZPbHWXo8WbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Генерация синтетических данных PyTorch\n",
        "X_torch = torch.rand(1000, 10)\n",
        "y_torch = torch.randint(0, 2, (1000,))\n",
        "\n",
        "# Преобразование в NumPy для LightGBM\n",
        "X_np_from_torch = X_torch.numpy()\n",
        "y_np_from_torch = y_torch.numpy()\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки (теперь в NumPy)\n",
        "X_train_np_t, X_test_np_t, y_train_np_t, y_test_np_t = train_test_split(X_np_from_torch, y_np_from_torch, test_size=0.2, random_state=42)\n",
        "\n",
        "# Инициализация и обучение LightGBM Classifier\n",
        "lgb_model_torch = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', random_state=42)\n",
        "lgb_model_torch.fit(X_train_np_t, y_train_np_t)\n",
        "\n",
        "# Предсказания\n",
        "y_pred_lgb_torch = lgb_model_torch.predict(X_test_np_t)\n",
        "accuracy_lgb_torch = accuracy_score(y_test_np_t, y_pred_lgb_torch)\n",
        "print(f\"LightGBM (PyTorch -> NumPy) Accuracy: {accuracy_lgb_torch:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "wSphPGW28WbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4\\. PyTorch Lightning\n",
        "\n",
        "PyTorch Lightning - это легковесная оболочка для PyTorch, которая обеспечивает высокоуровневый интерфейс для организации и масштабирования моделей PyTorch.\n",
        "\n",
        "**Что это делает:**\n",
        "PyTorch Lightning помогает инженерам по машинному обучению и исследователям:\n",
        "\n",
        "  * **Упорядочивать код:** Разделяет код исследования и инженерный код.\n",
        "  * **Уменьшать количество шаблонного кода:** Автоматизирует циклы обучения, логирование, сохранение контрольных точек и другие общие задачи.\n",
        "  * **Обеспечивать воспроизводимость:** Упрощает воспроизведение экспериментов.\n",
        "  * **Поддерживать масштабирование:** Легко переключается между CPU, GPU и распределенным обучением.\n",
        "\n",
        "**Установка:**\n",
        "`pip install pytorch-lightning`\n",
        "\n",
        "**Основные компоненты:**\n",
        "\n",
        "  * `LightningModule`: Основной класс, где вы определяете свою модель PyTorch, прямую передачу, шаг обучения, шаг валидации и оптимизатор.\n",
        "  * `Trainer`: Оркестратор, который обрабатывает цикл обучения, валидацию, тестирование и т.д.\n",
        "  * `Dataloader`: Стандартный PyTorch `DataLoader` для загрузки данных.\n",
        "  * `LightningDataModule`: Необязательный, но рекомендуемый способ инкапсуляции всей логики подготовки данных (загрузка, разделение, преобразование).\n",
        "\n",
        "**Примеры использования с NumPy и PyTorch:**\n",
        "\n",
        "PyTorch Lightning изначально работает с тензорами PyTorch, поэтому преобразование NumPy будет происходить на этапе создания `Dataset` и `DataLoader`.\n",
        "\n",
        "**Пример с PyTorch (Классификация):**"
      ],
      "metadata": {
        "id": "cmXkNhpH8WbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Создание LightningModule\n",
        "class SimpleClassifier(pl.LightningModule):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = accuracy_score(y.cpu(), preds.cpu())\n",
        "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
        "        self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        return {'val_loss': loss, 'val_acc': acc}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "# 2. Создание данных\n",
        "# Генерация синтетических данных NumPy\n",
        "X_np = np.random.rand(1000, 10).astype(np.float32)\n",
        "y_np = np.random.randint(0, 2, 1000).astype(np.int64)\n",
        "\n",
        "# Преобразование в тензоры PyTorch\n",
        "X_torch = torch.from_numpy(X_np)\n",
        "y_torch = torch.from_numpy(y_np)\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "X_train_torch, X_val_torch, y_train_torch, y_val_torch = train_test_split(X_torch, y_torch, test_size=0.2, random_state=42)\n",
        "\n",
        "# Создание PyTorch Datasets и DataLoaders\n",
        "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
        "val_dataset = TensorDataset(X_val_torch, y_val_torch)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# 3. Инициализация модели и тренера\n",
        "model = SimpleClassifier(input_dim=10, num_classes=2)\n",
        "trainer = pl.Trainer(max_epochs=5, enable_progress_bar=True, accelerator='cpu') # Используйте 'gpu' если доступно\n",
        "\n",
        "# 4. Обучение модели\n",
        "print(\"Начало обучения PyTorch Lightning...\")\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "print(\"Обучение PyTorch Lightning завершено.\")\n",
        "\n",
        "# (Необязательно) Тестирование модели после обучения\n",
        "# trainer.test(model, test_loader)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "CAOjIeSy8WbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Пример с NumPy (для Lightning нужно преобразовать в PyTorch):**\n",
        "\n",
        "Как показано выше, данные NumPy преобразуются в тензоры PyTorch, а затем используются с `TensorDataset` и `DataLoader`.\n",
        "\n",
        "### 5\\. Transformers (Hugging Face)\n",
        "\n",
        "Библиотека Hugging Face Transformers предоставляет тысячи предварительно обученных моделей для выполнения задач на различных модальностях, таких как текст, изображение и аудио.\n",
        "\n",
        "**Что это делает:**\n",
        "Она предлагает унифицированный API для использования, обучения и тонкой настройки моделей на основе трансформаторов, таких как BERT, GPT-2, T5, ViT и многих других. Это позволяет быстро экспериментировать с передовыми моделями глубокого обучения для NLP (обработка естественного языка), компьютерного зрения и аудио.\n",
        "\n",
        "**Установка:**\n",
        "`pip install transformers`\n",
        "\n",
        "**Основные компоненты:**\n",
        "\n",
        "  * `AutoTokenizer`: Загружает токенизатор, соответствующий предварительно обученной модели.\n",
        "  * `AutoModelForSequenceClassification`, `AutoModelForCausalLM` и т.д.: Загружает соответствующую предварительно обученную архитектуру модели для конкретной задачи.\n",
        "  * `pipeline`: Высокоуровневый API для быстрого использования моделей для общих задач (анализ настроений, генерация текста, вопросно-ответная система и т.д.).\n",
        "  * `Trainer`: Специализированный класс для обучения моделей Transformers, часто используемый с `pytorch-lightning` под капотом (или просто PyTorch).\n",
        "\n",
        "**Примеры использования (в основном с PyTorch):**\n",
        "\n",
        "Transformers глубоко интегрированы с PyTorch (и TensorFlow/JAX), поэтому примеры будут демонстрировать работу с тензорами PyTorch. NumPy используется для подготовки необработанных данных перед токенизацией.\n",
        "\n",
        "**Пример 1: Анализ настроений с использованием `pipeline` (высокоуровневый):**"
      ],
      "metadata": {
        "id": "tC1tKUGN8WbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Загрузка предварительно обученной модели для анализа настроений\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "texts = [\n",
        "    \"Я люблю эту библиотеку! Она очень крутая.\",\n",
        "    \"Это было довольно скучно.\",\n",
        "    \"Я не уверен, что об этом думать.\"\n",
        "]\n",
        "\n",
        "results = sentiment_analyzer(texts)\n",
        "for text, result in zip(texts, results):\n",
        "    print(f\"Текст: '{text}' -> Настроение: {result['label']} (Степень: {result['score']:.4f})\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "VOGUboPh8WbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Пример 2: Классификация текста с использованием `AutoModel` и `AutoTokenizer` (низкоуровневый):**"
      ],
      "metadata": {
        "id": "SXahJe_P8WbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Загрузка токенизатора и модели для классификации последовательностей\n",
        "# (Используем небольшую модель для демонстрации)\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Пример текста\n",
        "texts = [\n",
        "    \"Hugging Face Transformers - это потрясающе!\",\n",
        "    \"Я не наслаждался этим фильмом.\"\n",
        "]\n",
        "\n",
        "# Токенизация входных данных\n",
        "# `return_tensors='pt'` указывает на возвращение тензоров PyTorch\n",
        "encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Прямая передача через модель\n",
        "with torch.no_grad(): # Отключение вычисления градиентов для инференса\n",
        "    outputs = model(**encoded_input)\n",
        "\n",
        "# Получение логитов и применение softmax для получения вероятностей\n",
        "logits = outputs.logits\n",
        "probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "# Получение предсказанных классов\n",
        "predicted_class_ids = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "# Декодирование меток (для этой модели: 0 - отрицательный, 1 - положительный)\n",
        "id_to_label = model.config.id2label\n",
        "\n",
        "for i, text in enumerate(texts):\n",
        "    predicted_label = id_to_label[predicted_class_ids[i].item()]\n",
        "    print(f\"Текст: '{text}' -> Предсказанная метка: {predicted_label} (Вероятности: {probabilities[i]})\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8Ya1W2oF8WbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Пример с NumPy (для Transformers, нужно преобразовать в PyTorch/TensorFlow):**\n",
        "\n",
        "Непосредственно NumPy не используется для инференса или обучения в Transformers. Вместо этого данные NumPy преобразуются в тензоры PyTorch (или TensorFlow) после предварительной обработки (например, токенизации)."
      ],
      "metadata": {
        "id": "JAIBk-0Y8WbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Пример текстовых данных, представленных как массив NumPy (например, из CSV)\n",
        "# В реальном сценарии это были бы строки, а не случайные числа\n",
        "text_data_np = np.array([\n",
        "    \"This is a positive review.\",\n",
        "    \"I really disliked this product.\",\n",
        "    \"It's okay, not great.\"\n",
        "])\n",
        "\n",
        "# Токенизация текстовых данных из NumPy (они должны быть строками)\n",
        "encoded_input_np = tokenizer(text_data_np.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Продолжение как в примере PyTorch выше\n",
        "with torch.no_grad():\n",
        "    outputs = model(**encoded_input_np)\n",
        "\n",
        "logits = outputs.logits\n",
        "probabilities = torch.softmax(logits, dim=1)\n",
        "predicted_class_ids = torch.argmax(probabilities, dim=1)\n",
        "id_to_label = model.config.id2label\n",
        "\n",
        "for i, text in enumerate(text_data_np):\n",
        "    predicted_label = id_to_label[predicted_class_ids[i].item()]\n",
        "    print(f\"Текст: '{text}' -> Предсказанная метка: {predicted_label} (Вероятности: {probabilities[i]})\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "g_cPfhFe8WbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## Обучение на своем датасете\n",
        "\n",
        "Давайте пройдемся по общим шагам для обучения моделей на вашем собственном наборе данных. Мы сосредоточимся на каждом типе библиотеки.\n",
        "\n",
        "### Общие шаги для всех библиотек:\n",
        "\n",
        "1.  **Загрузка данных:** Прочтите ваш набор данных (CSV, TXT, JSON, изображения и т.д.) в подходящую структуру данных (Pandas DataFrame, NumPy array, списки строк).\n",
        "2.  **Разделение данных:** Разделите набор данных на обучающие, валидационные и/или тестовые выборки. Это важно для оценки производительности модели на невидимых данных.\n",
        "3.  **Предварительная обработка данных:**\n",
        "      * **Числовые данные:** Масштабирование (StandardScaler, MinMaxScaler), обработка пропущенных значений.\n",
        "      * **Категориальные данные:** One-hot encoding, Label encoding (CatBoost обрабатывает их сам).\n",
        "      * **Текстовые данные:** Токенизация, создание встраиваний, паддинг.\n",
        "      * **Изображения:** Изменение размера, нормализация, аугментация.\n",
        "4.  **Обучение модели:** Инициализируйте модель и обучите ее на обучающих данных.\n",
        "5.  **Оценка модели:** Оцените производительность модели на валидационных/тестовых данных с использованием соответствующих метрик.\n",
        "6.  **Настройка гиперпараметров (необязательно):** Используйте методы, такие как Grid Search, Random Search, Optuna или Ray Tune, чтобы найти оптимальные гиперпараметры.\n",
        "7.  **Сохранение/загрузка модели:** Сохраните обученную модель для будущего использования.\n",
        "\n",
        "### 1\\. Обучение XGBoost/CatBoost/LightGBM на своем датасете (Табличные данные)\n",
        "\n",
        "Предположим, у вас есть CSV-файл `my_dataset.csv` с табличными данными."
      ],
      "metadata": {
        "id": "Gzu8SftN8WbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "# Импортируйте нужную модель\n",
        "# import xgboost as xgb\n",
        "# from catboost import CatBoostClassifier\n",
        "# import lightgbm as lgb\n",
        "\n",
        "# 1. Загрузка данных\n",
        "try:\n",
        "    df = pd.read_csv('my_dataset.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"my_dataset.csv не найден. Создаю синтетический датасет для демонстрации.\")\n",
        "    # Создание синтетического датасета для демонстрации\n",
        "    data = {\n",
        "        'feature_A': np.random.rand(1000),\n",
        "        'feature_B': np.random.randint(0, 5, 1000), # Категориальный признак\n",
        "        'feature_C': np.random.randn(1000) * 10,\n",
        "        'target_class': np.random.randint(0, 2, 1000), # Для классификации\n",
        "        'target_reg': np.random.rand(1000) * 100 # Для регрессии\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv('my_dataset.csv', index=False)\n",
        "    print(\"my_dataset.csv создан.\")\n",
        "\n",
        "# Предположим, вы хотите предсказать 'target_class' (классификация)\n",
        "# Или 'target_reg' (регрессия)\n",
        "\n",
        "# Разделение признаков (X) и целевой переменной (y)\n",
        "X = df.drop(['target_class', 'target_reg'], axis=1) # Удалите те, которые не используете\n",
        "y_class = df['target_class']\n",
        "y_reg = df['target_reg']\n",
        "\n",
        "# Идентификация числовых и категориальных признаков\n",
        "numerical_cols = X.select_dtypes(include=np.number).columns\n",
        "categorical_cols = X.select_dtypes(include='object').columns # Если есть столбцы-объекты\n",
        "\n",
        "# 2. Предварительная обработка данных\n",
        "# Масштабирование числовых признаков (для XGBoost/LightGBM это не всегда строго необходимо,\n",
        "# но может помочь в некоторых случаях)\n",
        "scaler = StandardScaler()\n",
        "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
        "\n",
        "# Для категориальных признаков:\n",
        "# CatBoost обрабатывает их сам, но для XGBoost/LightGBM вам может понадобиться кодирование\n",
        "# Пример One-Hot Encoding для XGBoost/LightGBM\n",
        "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# 3. Разделение данных\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=0.2, random_state=42)\n",
        "# Или для регрессии:\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Размер обучающей выборки: {X_train.shape}\")\n",
        "print(f\"Размер тестовой выборки: {X_test.shape}\")\n",
        "\n",
        "# 4. Обучение модели\n",
        "\n",
        "# --- Пример с XGBoost (классификация) ---\n",
        "print(\"\\nОбучение XGBoost...\")\n",
        "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "print(f\"XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
        "\n",
        "# --- Пример с CatBoost (классификация) ---\n",
        "print(\"\\nОбучение CatBoost...\")\n",
        "# CatBoost предпочитает необработанные категориальные данные или явное указание\n",
        "# Если вы уже использовали pd.get_dummies, то CatBoost будет воспринимать их как числовые.\n",
        "# Если вы хотите, чтобы CatBoost обрабатывал категориальные признаки,\n",
        "# то не делайте get_dummies до CatBoost, а просто передайте categorical_features_indices\n",
        "cat_model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6,\n",
        "                               loss_function='Logloss', eval_metric='Accuracy',\n",
        "                               random_seed=42, verbose=False)\n",
        "# Пример с категориальным столбцом:\n",
        "# Если 'feature_B' была бы объектом/строкой до get_dummies:\n",
        "# cat_features_indices = [X.columns.get_loc('feature_B')] # Пример для одной колонки\n",
        "# cat_model.fit(X_train, y_train, cat_features=cat_features_indices)\n",
        "cat_model.fit(X_train, y_train) # Если категориальные уже закодированы (one-hot)\n",
        "y_pred_cat = cat_model.predict(X_test)\n",
        "print(f\"CatBoost Accuracy: {accuracy_score(y_test, y_pred_cat):.4f}\")\n",
        "\n",
        "\n",
        "# --- Пример с LightGBM (классификация) ---\n",
        "print(\"\\nОбучение LightGBM...\")\n",
        "lgb_model = lgb.LGBMClassifier(objective='binary', metric='binary_logloss', random_state=42)\n",
        "lgb_model.fit(X_train, y_train)\n",
        "y_pred_lgb = lgb_model.predict(X_test)\n",
        "print(f\"LightGBM Accuracy: {accuracy_score(y_test, y_pred_lgb):.4f}\")\n",
        "\n",
        "# 5. Сохранение/Загрузка модели\n",
        "# Для XGBoost\n",
        "# xgb_model.save_model(\"xgb_model.json\")\n",
        "# loaded_xgb_model = xgb.XGBClassifier()\n",
        "# loaded_xgb_model.load_model(\"xgb_model.json\")\n",
        "\n",
        "# Для CatBoost\n",
        "# cat_model.save_model(\"cat_model.cbm\")\n",
        "# loaded_cat_model = CatBoostClassifier()\n",
        "# loaded_cat_model.load_model(\"cat_model.cbm\")\n",
        "\n",
        "# Для LightGBM\n",
        "# lgb_model.booster_.save_model(\"lgbm_model.txt\")\n",
        "# loaded_lgb_model = lgb.Booster(model_file=\"lgbm_model.txt\")\n",
        "# # Для предсказаний с загруженным бустером: loaded_lgb_model.predict(X_test)\n",
        "# # Если вы хотите использовать API LGBMClassifier, загружайте так:\n",
        "# loaded_lgb_classifier = lgb.LGBMClassifier()\n",
        "# loaded_lgb_classifier.booster_ = lgb.Booster(model_file=\"lgbm_model.txt\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "9IW0c-Mb8WbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2\\. Обучение PyTorch Lightning на своем датасете\n",
        "\n",
        "Предположим, у вас есть CSV-файл с числовыми данными для классификации."
      ],
      "metadata": {
        "id": "3hop2yin8WbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "\n",
        "# 1. Загрузка данных (синтетический пример)\n",
        "try:\n",
        "    df_pl = pd.read_csv('my_dataset_pl.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"my_dataset_pl.csv не найден. Создаю синтетический датасет для демонстрации.\")\n",
        "    data_pl = {\n",
        "        'feature_1': np.random.rand(1000).astype(np.float32),\n",
        "        'feature_2': np.random.rand(1000).astype(np.float32),\n",
        "        'feature_3': np.random.rand(1000).astype(np.float32),\n",
        "        'target': np.random.randint(0, 2, 1000).astype(np.int64)\n",
        "    }\n",
        "    df_pl = pd.DataFrame(data_pl)\n",
        "    df_pl.to_csv('my_dataset_pl.csv', index=False)\n",
        "    print(\"my_dataset_pl.csv создан.\")\n",
        "\n",
        "X_pl = df_pl.drop('target', axis=1)\n",
        "y_pl = df_pl['target']\n",
        "\n",
        "# 2. Предварительная обработка данных\n",
        "scaler_pl = StandardScaler()\n",
        "X_scaled_pl = scaler_pl.fit_transform(X_pl)\n",
        "\n",
        "# Преобразование в тензоры PyTorch\n",
        "X_torch_pl = torch.from_numpy(X_scaled_pl)\n",
        "y_torch_pl = torch.from_numpy(y_pl.values)\n",
        "\n",
        "# 3. Разделение данных\n",
        "X_train_pl, X_val_pl, y_train_pl, y_val_pl = train_test_split(X_torch_pl, y_torch_pl, test_size=0.2, random_state=42)\n",
        "\n",
        "# Создание PyTorch Datasets и DataLoaders\n",
        "train_dataset_pl = TensorDataset(X_train_pl, y_train_pl)\n",
        "val_dataset_pl = TensorDataset(X_val_pl, y_val_pl)\n",
        "\n",
        "train_loader_pl = DataLoader(train_dataset_pl, batch_size=32, shuffle=True)\n",
        "val_loader_pl = DataLoader(val_dataset_pl, batch_size=32)\n",
        "\n",
        "# 4. Определение LightningModule (как в примере выше)\n",
        "class MyClassifier(pl.LightningModule):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.loss_fn(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = accuracy_score(y.cpu(), preds.cpu())\n",
        "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
        "        self.log('val_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        return {'val_loss': loss, 'val_acc': acc}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "# Инициализация модели и тренера\n",
        "input_dim = X_train_pl.shape[1]\n",
        "num_classes = len(y_pl.unique())\n",
        "model_pl = MyClassifier(input_dim=input_dim, num_classes=num_classes)\n",
        "trainer_pl = pl.Trainer(max_epochs=5, enable_progress_bar=True, accelerator='cpu') # Используйте 'gpu' если доступно\n",
        "\n",
        "# 5. Обучение модели\n",
        "print(\"\\nНачало обучения PyTorch Lightning на своем датасете...\")\n",
        "trainer_pl.fit(model_pl, train_loader_pl, val_loader_pl)\n",
        "print(\"Обучение PyTorch Lightning завершено.\")\n",
        "\n",
        "# 6. Сохранение/Загрузка модели\n",
        "# trainer_pl.save_checkpoint(\"my_lightning_model.ckpt\")\n",
        "# loaded_model_pl = MyClassifier.load_from_checkpoint(\"my_lightning_model.ckpt\", input_dim=input_dim, num_classes=num_classes)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "FQL_Yt-48WbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3\\. Обучение Transformers на своем датасете (Классификация текста)\n",
        "\n",
        "Предположим, у вас есть CSV-файл `my_text_dataset.csv` с двумя столбцами: `text` и `label`."
      ],
      "metadata": {
        "id": "1GAcQUVK8WbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset # Hugging Face datasets library\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "\n",
        "# 1. Загрузка данных (синтетический пример)\n",
        "try:\n",
        "    df_hf = pd.read_csv('my_text_dataset.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"my_text_dataset.csv не найден. Создаю синтетический датасет для демонстрации.\")\n",
        "    texts_hf = [\n",
        "        \"Я абсолютно люблю этот продукт, он великолепен!\",\n",
        "        \"Это было худшее впечатление в моей жизни.\",\n",
        "        \"Немного скучно, но не ужасно.\",\n",
        "        \"Очень понравилось, определенно рекомендую.\",\n",
        "        \"Это просто мусор, не покупайте.\"\n",
        "    ]\n",
        "    labels_hf = [1, 0, 1, 1, 0] # 1 - положительный, 0 - отрицательный\n",
        "\n",
        "    df_hf = pd.DataFrame({'text': texts_hf, 'label': labels_hf})\n",
        "    df_hf.to_csv('my_text_dataset.csv', index=False)\n",
        "    print(\"my_text_dataset.csv создан.\")\n",
        "\n",
        "# 2. Разделение данных\n",
        "train_df_hf, test_df_hf = train_test_split(df_hf, test_size=0.2, random_state=42)\n",
        "\n",
        "# Преобразование Pandas DataFrame в Hugging Face Dataset\n",
        "train_dataset_hf = Dataset.from_pandas(train_df_hf, preserve_index=False)\n",
        "test_dataset_hf = Dataset.from_pandas(test_df_hf, preserve_index=False)\n",
        "\n",
        "# 3. Предварительная обработка данных (Токенизация)\n",
        "model_checkpoint = \"distilbert-base-uncased\" # Выберите модель, которую хотите использовать\n",
        "tokenizer_hf = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer_hf(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Применение токенизации к датасетам\n",
        "tokenized_train_dataset = train_dataset_hf.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset_hf.map(tokenize_function, batched=True)\n",
        "\n",
        "# Удаление исходного текстового столбца, так как он больше не нужен\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"text\"])\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"text\"])\n",
        "\n",
        "# Назначение правильного формата для PyTorch\n",
        "tokenized_train_dataset.set_format(\"torch\")\n",
        "tokenized_test_dataset.set_format(\"torch\")\n",
        "\n",
        "# Создание словаря map ID к метке и наоборот\n",
        "label_names = df_hf['label'].unique().tolist()\n",
        "label_names.sort() # Убедитесь, что метки отсортированы\n",
        "id2label = {idx: label for idx, label in enumerate(label_names)}\n",
        "label2id = {label: idx for idx, label in enumerate(label_names)}\n",
        "\n",
        "# 4. Инициализация модели и Trainer\n",
        "model_hf = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=len(label_names), id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "# Определение аргументов для обучения\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\", # Оценка после каждой эпохи\n",
        "    save_strategy=\"epoch\",       # Сохранение после каждой эпохи\n",
        "    load_best_model_at_end=True, # Загрузка лучшей модели после обучения\n",
        "    metric_for_best_model=\"accuracy\",\n",
        ")\n",
        "\n",
        "# Определение функции для вычисления метрик\n",
        "from datasets import load_metric\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Инициализация Trainer\n",
        "trainer_hf = Trainer(\n",
        "    model=model_hf,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset, # Используем тестовый набор для оценки\n",
        "    tokenizer=tokenizer_hf,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 5. Обучение модели\n",
        "print(\"\\nНачало обучения Transformers на своем датасете...\")\n",
        "trainer_hf.train()\n",
        "print(\"Обучение Transformers завершено.\")\n",
        "\n",
        "# 6. Оценка модели на тестовом наборе\n",
        "results = trainer_hf.evaluate()\n",
        "print(f\"Оценка на тестовом наборе: {results}\")\n",
        "\n",
        "# 7. Сохранение/Загрузка модели\n",
        "trainer_hf.save_model(\"./my_trained_transformer_model\")\n",
        "# Чтобы загрузить:\n",
        "# loaded_tokenizer = AutoTokenizer.from_pretrained(\"./my_trained_transformer_model\")\n",
        "# loaded_model = AutoModelForSequenceClassification.from_pretrained(\"./my_trained_transformer_model\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "7gDE5rFU8WbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "Это всестороннее руководство должно дать вам прочную основу для начала работы с каждой из этих мощных библиотек и обучения их на ваших собственных данных. Не стесняйтесь задавать вопросы, если вам нужны более конкретные примеры или уточнения\\!"
      ],
      "metadata": {
        "id": "ka5g8bpR8WbW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}