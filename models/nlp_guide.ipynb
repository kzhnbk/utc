{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "О, отлично\\! Вы выбрали мощные библиотеки для обработки естественного языка (NLP). Давайте разберем каждую из них, покажем основные методы и примеры использования с NumPy и PyTorch.\n",
        "\n",
        "### Общие положения\n",
        "\n",
        "Прежде чем мы углубимся в детали, вот несколько общих моментов, которые будут актуальны для всех библиотек:\n",
        "\n",
        "  * **Токенизация (Tokenization):** Разделение текста на более мелкие единицы, такие как слова или предложения.\n",
        "  * **Лемматизация/Стемминг (Lemmatization/Stemming):** Приведение слов к их базовой форме. Лемматизация (например, \"бегу\", \"бежал\" -\\> \"бежать\") обычно предпочтительнее стемминга (\"бегу\", \"бежал\" -\\> \"бег\"), так как сохраняет грамматическую корректность.\n",
        "  * **Стоп-слова (Stop Words):** Общие слова (например, \"и\", \"в\", \"на\"), которые часто отфильтровываются, так как они не несут много смысла для анализа.\n",
        "  * **Векторные представления слов (Word Embeddings):** Числовые представления слов, которые захватывают их семантические отношения. Это ключевой элемент для использования NumPy и PyTorch.\n",
        "\n",
        "Давайте начнем\\!\n",
        "\n",
        "-----\n",
        "\n",
        "## 1\\. SpaCy\n",
        "\n",
        "**Что это?** SpaCy - это библиотека для обработки естественного языка, разработанная для высокопроизводительной обработки. Она предоставляет готовые к использованию модели для различных языков, что делает ее очень удобной для быстрого прототипирования и продакшн-систем. Она особенно хороша для:\n",
        "\n",
        "  * Именованных сущностей (NER)\n",
        "  * Морфологического анализа (PoS-tagging)\n",
        "  * Синтаксического анализа (Dependency Parsing)\n",
        "  * Лемматизации\n",
        "  * Токенизации\n",
        "\n",
        "**Установка:**\n",
        "\n",
        "```bash\n",
        "pip install spacy\n",
        "python -m spacy download en_core_web_sm # Скачивание маленькой модели для английского\n",
        "python -m spacy download ru_core_web_sm # Если есть необходимость в русской модели (для примера)\n",
        "```\n",
        "\n",
        "**Основные методы и примеры:**\n",
        "\n",
        "Для SpaCy вам нужно загрузить языковую модель."
      ],
      "metadata": {
        "id": "2g1yke_G7Onu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Загрузка языковой модели\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"--- SpaCy: Основные возможности ---\")\n",
        "\n",
        "# 1. Токенизация\n",
        "print(\"\\n1. Токенизация:\")\n",
        "for token in doc:\n",
        "    print(token.text)\n",
        "\n",
        "# 2. Морфологический анализ (PoS-tagging)\n",
        "print(\"\\n2. Морфологический анализ (PoS-tagging):\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text} -> {token.pos_} ({token.tag_})\") # pos_ - универсальный тег, tag_ - специфичный для дерева разбора\n",
        "\n",
        "# 3. Лемматизация\n",
        "print(\"\\n3. Лемматизация:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text} -> {token.lemma_}\")\n",
        "\n",
        "# 4. Именованные сущности (NER)\n",
        "print(\"\\n4. Именованные сущности (NER):\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} -> {ent.label_}\")\n",
        "\n",
        "# 5. Синтаксический анализ (Dependency Parsing)\n",
        "print(\"\\n5. Синтаксический анализ (Dependency Parsing):\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text} --({token.dep_})--> {token.head.text}\")\n",
        "\n",
        "# 6. Доступ к векторным представлениям слов (Word Vectors)\n",
        "# SpaCy модели обычно поставляются с предобученными векторами слов.\n",
        "# Если вы используете модель без векторов (например, `en_core_web_sm` имеет ограниченные векторы,\n",
        "# а `en_core_web_lg` - более полные), вам нужно будет загрузить модель с векторами.\n",
        "# Например: nlp = spacy.load(\"en_core_web_lg\")\n",
        "# Если модель не содержит векторов, `has_vector` будет False, и `vector` будет пустым.\n",
        "\n",
        "print(\"\\n6. Векторные представления слов:\")\n",
        "for token in doc:\n",
        "    if token.has_vector:\n",
        "        print(f\"Слово: '{token.text}', Вектор (первые 5 элементов): {token.vector[:5]}\")\n",
        "        # Пример использования с NumPy\n",
        "        numpy_vector = token.vector\n",
        "        print(f\"Тип NumPy вектора: {type(numpy_vector)}, Форма: {numpy_vector.shape}\")\n",
        "\n",
        "        # Пример использования с PyTorch\n",
        "        if token.has_vector: # Проверка на наличие вектора\n",
        "            torch_tensor = torch.from_numpy(token.vector)\n",
        "            print(f\"Тип PyTorch тензора: {type(torch_tensor)}, Форма: {torch_tensor.shape}\")\n",
        "            print(f\"PyTorch тензор (первые 5 элементов): {torch_tensor[:5]}\")\n",
        "        else:\n",
        "            print(f\"Слово '{token.text}' не имеет вектора.\")\n",
        "    else:\n",
        "        print(f\"Слово '{token.text}' не имеет вектора (возможно, модель 'sm' не содержит полных векторов).\")\n",
        "\n",
        "# Пример сравнения сходства документов\n",
        "print(\"\\n7. Сравнение сходства документов:\")\n",
        "doc1 = nlp(\"I like to eat apples.\")\n",
        "doc2 = nlp(\"I enjoy eating fruit.\")\n",
        "doc3 = nlp(\"The cat sat on the mat.\")\n",
        "\n",
        "# Для корректного сравнения сходства нужны модели с обученными векторами.\n",
        "# Если вы используете модель 'sm', сходство может быть неточным.\n",
        "# Для лучшего результата используйте 'en_core_web_md' или 'en_core_web_lg'.\n",
        "# python -m spacy download en_core_web_md\n",
        "\n",
        "try:\n",
        "    print(f\"Сходство 'apples' и 'fruit': {nlp('apples').similarity(nlp('fruit'))}\")\n",
        "    print(f\"Сходство 'apples' и 'cat': {nlp('apples').similarity(nlp('cat'))}\")\n",
        "    print(f\"Сходство doc1 и doc2: {doc1.similarity(doc2)}\")\n",
        "    print(f\"Сходство doc1 и doc3: {doc1.similarity(doc3)}\")\n",
        "except NotImplementedError:\n",
        "    print(\"Ошибка: Модель SpaCy не имеет векторов слов для сравнения сходства. Попробуйте загрузить более крупную модель (например, 'en_core_web_md').\")\n",
        "\n",
        "# Вы можете обновить модель для SpaCy до en_core_web_md или en_core_web_lg для лучших векторов:\n",
        "# python -m spacy download en_core_web_md\n",
        "# nlp = spacy.load(\"en_core_web_md\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "kFUmh86k7On0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## 2\\. NLTK (Natural Language Toolkit)\n",
        "\n",
        "**Что это?** NLTK - это ведущая платформа для создания программ Python, работающих с человеческим языком. Она предоставляет простой в использовании интерфейс для более чем 50 корпусов и лексических ресурсов (таких как WordNet), набор библиотек для обработки текста для классификации, токенизации, стемминга, тегирования, синтаксического анализа и семантического рассуждения, а также обертки для промышленных библиотек NLP. NLTK чаще используется для исследовательских и образовательных целей из-за своей гибкости и модульности.\n",
        "\n",
        "**Установка:**\n",
        "\n",
        "```bash\n",
        "pip install nltk\n",
        "```\n",
        "\n",
        "**Скачивание необходимых данных (однократно):**"
      ],
      "metadata": {
        "id": "aUtD29LW7On5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')      # Для токенизации\n",
        "nltk.download('wordnet')    # Для лемматизации\n",
        "nltk.download('averaged_perceptron_tagger') # Для PoS-теггинга\n",
        "nltk.download('stopwords')  # Для стоп-слов"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "5NUafd5p7On6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Основные методы и примеры:**"
      ],
      "metadata": {
        "id": "dPq6ugl_7On7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "text = \"NLTK is a powerful library for natural language processing. It provides many functionalities.\"\n",
        "\n",
        "print(\"--- NLTK: Основные возможности ---\")\n",
        "\n",
        "# 1. Токенизация\n",
        "print(\"\\n1. Токенизация:\")\n",
        "words = word_tokenize(text)\n",
        "sentences = sent_tokenize(text)\n",
        "print(f\"Слова: {words}\")\n",
        "print(f\"Предложения: {sentences}\")\n",
        "\n",
        "# 2. Стемминг (Porter Stemmer)\n",
        "print(\"\\n2. Стемминг:\")\n",
        "ps = PorterStemmer()\n",
        "stemmed_words = [ps.stem(w) for w in words]\n",
        "print(f\"Исходные слова: {words}\")\n",
        "print(f\"Простемленные слова: {stemmed_words}\")\n",
        "\n",
        "# 3. Лемматизация (WordNet Lemmatizer)\n",
        "print(\"\\n3. Лемматизация:\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(w) for w in words]\n",
        "print(f\"Исходные слова: {words}\")\n",
        "print(f\"Лемматизированные слова: {lemmatized_words}\")\n",
        "# Лемматизация часто требует POS-тега для лучшей точности\n",
        "# Например, 'running' может быть глаголом или существительным.\n",
        "print(f\"Лемматизация 'running' (глагол): {lemmatizer.lemmatize('running', pos='v')}\")\n",
        "print(f\"Лемматизация 'running' (существительное): {lemmatizer.lemmatize('running', pos='n')}\")\n",
        "\n",
        "\n",
        "# 4. Стоп-слова\n",
        "print(\"\\n4. Стоп-слова:\")\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [w for w in words if w.lower() not in stop_words]\n",
        "print(f\"Исходные слова: {words}\")\n",
        "print(f\"Отфильтрованные слова (без стоп-слов): {filtered_words}\")\n",
        "\n",
        "# 5. Морфологический анализ (PoS-tagging)\n",
        "print(\"\\n5. Морфологический анализ (PoS-tagging):\")\n",
        "pos_tags = pos_tag(words)\n",
        "print(f\"PoS-теги: {pos_tags}\")\n",
        "\n",
        "# 6. Пример использования с NumPy и PyTorch (для NLTK это обычно делается через Word Embeddings,\n",
        "# которые NLTK сам по себе не генерирует, но может использовать внешние)\n",
        "# Для демонстрации, давайте представим, что у нас есть некоторое словарное сопоставление с индексами.\n",
        "\n",
        "print(\"\\n6. Пример с NumPy и PyTorch (с использованием гипотетических векторов):\")\n",
        "vocab = {'NLTK': 0, 'powerful': 1, 'library': 2, 'natural': 3, 'language': 4, 'processing': 5, '.': 6, 'It': 7, 'provides': 8, 'many': 9, 'functionalities': 10}\n",
        "# Представим, что у нас есть некоторые предобученные векторные представления для этих слов.\n",
        "# В реальном сценарии вы бы использовали Gensim, SpaCy или другие библиотеки для получения эмбеддингов.\n",
        "embedding_dim = 10\n",
        "word_embeddings = np.random.rand(len(vocab), embedding_dim) # Случайные векторы для примера\n",
        "\n",
        "# Преобразование слов в векторы\n",
        "vectors = []\n",
        "for word in words:\n",
        "    if word in vocab:\n",
        "        vectors.append(word_embeddings[vocab[word]])\n",
        "    else:\n",
        "        vectors.append(np.zeros(embedding_dim)) # Если слово не найдено, используем нулевой вектор\n",
        "\n",
        "# Использование с NumPy\n",
        "numpy_vectors = np.array(vectors)\n",
        "print(f\"NumPy представление слов: \\n{numpy_vectors}\")\n",
        "print(f\"Форма NumPy массива: {numpy_vectors.shape}\")\n",
        "\n",
        "# Использование с PyTorch\n",
        "torch_tensor = torch.tensor(vectors, dtype=torch.float32)\n",
        "print(f\"PyTorch тензор слов: \\n{torch_tensor}\")\n",
        "print(f\"Форма PyTorch тензора: {torch_tensor.shape}\")\n",
        "\n",
        "# Пример использования для очень простого \"векторизатора\" предложения\n",
        "# Суммирование векторов слов для получения вектора предложения\n",
        "sentence_vector_np = np.sum(numpy_vectors, axis=0)\n",
        "sentence_vector_torch = torch.sum(torch_tensor, dim=0)\n",
        "\n",
        "print(f\"Вектор предложения (NumPy, первые 5 элементов): {sentence_vector_np[:5]}\")\n",
        "print(f\"Вектор предложения (PyTorch, первые 5 элементов): {sentence_vector_torch[:5]}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "rAS-xnpI7On8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## 3\\. Gensim\n",
        "\n",
        "**Что это?** Gensim - это библиотека для тематического моделирования, векторного представления документов и схожести на основе корпуса, ориентированная на эффективность и масштабируемость. Она особенно хорошо подходит для:\n",
        "\n",
        "  * **Тематического моделирования (LDA, LSI, HDP):** Выявление скрытых тем в большом объеме текста.\n",
        "  * **Векторных представлений слов (Word2Vec, Doc2Vec, FastText):** Обучение векторных представлений слов и документов.\n",
        "  * **Сходства (Similarity Queries):** Поиск похожих документов или слов.\n",
        "\n",
        "**Установка:**\n",
        "\n",
        "```bash\n",
        "pip install gensim\n",
        "```\n",
        "\n",
        "**Основные методы и примеры:**"
      ],
      "metadata": {
        "id": "JUOi9akm7On9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from gensim.models.phrases import Phraser, Phrases\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LsiModel, LdaModel\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Пример данных\n",
        "documents = [\n",
        "    \"Human machine interface for lab abc computer applications\",\n",
        "    \"A survey of user opinion of computer system response time\",\n",
        "    \"The EPS user interface management system\",\n",
        "    \"System and human system engineering testing of EPS\",\n",
        "    \"Relation of user perceived response time to error measurement\",\n",
        "    \"The generation of random binary unordered trees\",\n",
        "    \"The intersection of graph theory and number theory\",\n",
        "    \"Human computer interaction\",\n",
        "    \"Computer science and engineering\",\n",
        "    \"Data mining and machine learning\",\n",
        "    \"Natural language processing and artificial intelligence\"\n",
        "]\n",
        "\n",
        "# Предварительная обработка (токенизация, приведение к нижнему регистру, удаление стоп-слов)\n",
        "texts = [[word for word in document.lower().split() if word.isalpha()] for document in documents]\n",
        "\n",
        "print(\"--- Gensim: Основные возможности ---\")\n",
        "\n",
        "# 1. Словарь (Dictionary)\n",
        "# Создание словаря из корпуса. Каждому уникальному слову присваивается ID.\n",
        "print(\"\\n1. Словарь (Dictionary):\")\n",
        "dictionary = Dictionary(texts)\n",
        "print(f\"Словарь: {dictionary.token2id}\")\n",
        "print(f\"Размер словаря: {len(dictionary)}\")\n",
        "\n",
        "# 2. Корпус Bag-of-Words (BoW)\n",
        "# Преобразование текстов в формат \"мешка слов\" (список пар (ID слова, количество появлений)).\n",
        "print(\"\\n2. Корпус Bag-of-Words (BoW):\")\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "print(f\"Пример BoW для первого документа: {corpus[0]}\")\n",
        "\n",
        "# 3. Тематическое моделирование (LDA - Latent Dirichlet Allocation)\n",
        "# LDA - это вероятностная модель, которая позволяет обнаружить \"темы\" в коллекции документов.\n",
        "# Каждая тема представляет собой распределение слов. Каждый документ является распределением тем.\n",
        "print(\"\\n3. Тематическое моделирование (LDA):\")\n",
        "num_topics = 2\n",
        "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
        "print(\"Темы LDA:\")\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f\"Тема {idx}: {topic}\")\n",
        "\n",
        "# Получение распределения тем для документа\n",
        "doc_topics = lda_model[corpus[0]]\n",
        "print(f\"Распределение тем для первого документа: {doc_topics}\")\n",
        "\n",
        "# Пример использования с NumPy и PyTorch для LDA:\n",
        "# Представление тем и распределений в NumPy/PyTorch\n",
        "# Матрица тем: num_topics x vocab_size (распределение слов по темам)\n",
        "lda_topics_np = lda_model.get_topics()\n",
        "print(f\"\\nLDA Темы (NumPy, форма): {lda_topics_np.shape}\")\n",
        "lda_topics_torch = torch.from_numpy(lda_topics_np)\n",
        "print(f\"LDA Темы (PyTorch, форма): {lda_topics_torch.shape}\")\n",
        "\n",
        "# Представление распределений тем для документов\n",
        "# doc_topic_dist - это распределение по темам для каждого документа\n",
        "doc_topic_dists = []\n",
        "for doc_bow in corpus:\n",
        "    doc_topic_dists.append([topic_prob for topic_id, topic_prob in lda_model.get_document_topics(doc_bow, minimum_probability=0)])\n",
        "doc_topic_dists_np = np.array(doc_topic_dists)\n",
        "print(f\"Распределения тем документов (NumPy, форма): {doc_topic_dists_np.shape}\")\n",
        "doc_topic_dists_torch = torch.from_numpy(doc_topic_dists_np)\n",
        "print(f\"Распределения тем документов (PyTorch, форма): {doc_topic_dists_torch.shape}\")\n",
        "\n",
        "\n",
        "# 4. Word2Vec\n",
        "# Обучение векторных представлений слов.\n",
        "# Words in context are similar, words with similar context are similar.\n",
        "print(\"\\n4. Word2Vec:\")\n",
        "# Параметры: vector_size (размерность эмбеддинга), window (размер окна), min_count (минимум появлений слова)\n",
        "word2vec_model = Word2Vec(texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "word2vec_model.train(texts, total_examples=word2vec_model.corpus_count, epochs=10)\n",
        "\n",
        "print(\"Вектор для слова 'computer':\")\n",
        "if 'computer' in word2vec_model.wv:\n",
        "    print(word2vec_model.wv['computer'][:5]) # Первые 5 элементов\n",
        "    # Использование с NumPy\n",
        "    computer_vec_np = word2vec_model.wv['computer']\n",
        "    print(f\"Тип NumPy вектора: {type(computer_vec_np)}, Форма: {computer_vec_np.shape}\")\n",
        "    # Использование с PyTorch\n",
        "    computer_vec_torch = torch.from_numpy(computer_vec_np)\n",
        "    print(f\"Тип PyTorch тензора: {type(computer_vec_torch)}, Форма: {computer_vec_torch.shape}\")\n",
        "else:\n",
        "    print(\"'computer' не найден в словаре модели Word2Vec.\")\n",
        "\n",
        "\n",
        "print(\"\\nСамые похожие слова на 'computer':\")\n",
        "if 'computer' in word2vec_model.wv:\n",
        "    print(word2vec_model.wv.most_similar('computer'))\n",
        "\n",
        "# 5. FastText (расширение Word2Vec, учитывает подслова)\n",
        "# Особенно полезен для редких слов и языков с богатой морфологией.\n",
        "print(\"\\n5. FastText:\")\n",
        "fasttext_model = FastText(texts, vector_size=100, window=5, min_count=1, workers=4, sg=1) # sg=1 для skip-gram\n",
        "fasttext_model.train(texts, total_examples=fasttext_model.corpus_count, epochs=10)\n",
        "\n",
        "print(\"Вектор для слова 'computer' (FastText):\")\n",
        "if 'computer' in fasttext_model.wv:\n",
        "    print(fasttext_model.wv['computer'][:5])\n",
        "\n",
        "print(\"\\nСамые похожие слова на 'computer' (FastText):\")\n",
        "if 'computer' in fasttext_model.wv:\n",
        "    print(fasttext_model.wv.most_similar('computer'))\n",
        "\n",
        "# FastText может генерировать векторы для OOV (Out-Of-Vocabulary) слов\n",
        "print(\"\\nВектор для 'computers' (FastText - OOV, если не было в тексте):\")\n",
        "if 'computers' not in fasttext_model.wv:\n",
        "    print(fasttext_model.wv['computers'][:5])\n",
        "else:\n",
        "    print(\"'computers' было в словаре модели FastText.\")\n",
        "\n",
        "\n",
        "# 6. Обнаружение фраз (Phrases)\n",
        "# Gensim может обнаруживать устойчивые фразы (например, \"New York\").\n",
        "print(\"\\n6. Обнаружение фраз:\")\n",
        "bigram = Phrases(texts, min_count=1, threshold=2) # threshold - чем больше, тем меньше фраз\n",
        "bigram_phraser = Phraser(bigram)\n",
        "texts_with_phrases = [bigram_phraser[text] for text in texts]\n",
        "print(f\"Оригинальный первый документ: {texts[0]}\")\n",
        "print(f\"Первый документ с фразами: {texts_with_phrases[0]}\")\n",
        "\n",
        "# Если вы обучите Word2Vec/FastText на `texts_with_phrases`,\n",
        "# вы получите векторы для таких фраз как \"human_computer\"."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "iv5F1Dnt7On-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## 4\\. FastText (отдельная библиотека Facebook)\n",
        "\n",
        "**Что это?** FastText - это библиотека с открытым исходным кодом от Facebook для эффективной классификации текста и обучения векторных представлений слов. Она основана на идеях Word2Vec, но с ключевым отличием: она рассматривает слова как \"мешок\" символьных N-грамм (подслов). Это позволяет ей:\n",
        "\n",
        "  * Создавать хорошие векторные представления для редких слов (которые встречаются мало раз, но их подслова могут быть частыми).\n",
        "  * Обрабатывать Out-Of-Vocabulary (OOV) слова, генерируя для них векторы, даже если они не были в обучающем корпусе.\n",
        "  * Быстро обучать модели классификации текста.\n",
        "\n",
        "Обратите внимание, что Gensim включает реализацию FastText, но здесь мы рассматриваем *отдельную* библиотеку `fasttext`.\n",
        "\n",
        "**Установка:**\n",
        "\n",
        "```bash\n",
        "pip install fasttext\n",
        "```\n",
        "\n",
        "**Основные методы и примеры:**\n",
        "\n",
        "Для обучения модели FastText требуется текстовый файл в определенном формате (одна строка - один документ, с метками класса, если это классификация)."
      ],
      "metadata": {
        "id": "jre_LCVU7On_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Создадим фиктивный файл для обучения\n",
        "train_data_file = \"train_data.txt\"\n",
        "with open(train_data_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"__label__positive This is a great movie.\\n\")\n",
        "    f.write(\"__label__negative This movie was terrible.\\n\")\n",
        "    f.write(\"__label__positive I really enjoyed the film.\\n\")\n",
        "    f.write(\"__label__negative What a waste of time.\\n\")\n",
        "    f.write(\"I like natural language processing.\\n\") # Без метки для обучения только эмбеддингов\n",
        "    f.write(\"I enjoy machine learning.\\n\")\n",
        "\n",
        "print(\"--- FastText (отдельная библиотека): Основные возможности ---\")\n",
        "\n",
        "# 1. Обучение модели Word Embeddings (без классификации)\n",
        "print(\"\\n1. Обучение Word Embeddings:\")\n",
        "# Обучение модели для получения векторов слов\n",
        "# input: текстовый файл\n",
        "# dim: размерность векторов\n",
        "# minn, maxn: минимальная и максимальная длина n-грамм символов\n",
        "word_model = fasttext.train_unsupervised(train_data_file, model='skipgram', dim=100, minn=2, maxn=5)\n",
        "\n",
        "# Получение вектора слова\n",
        "word = \"movie\"\n",
        "if word in word_model: # FastText напрямую не поддерживает проверку \"in\", но get_word_vector вернет вектор\n",
        "    vector_movie = word_model.get_word_vector(word)\n",
        "    print(f\"Вектор для '{word}' (первые 5 элементов): {vector_movie[:5]}\")\n",
        "    # Использование с NumPy\n",
        "    movie_vec_np = vector_movie\n",
        "    print(f\"Тип NumPy вектора: {type(movie_vec_np)}, Форма: {movie_vec_np.shape}\")\n",
        "    # Использование с PyTorch\n",
        "    movie_vec_torch = torch.from_numpy(movie_vec_np)\n",
        "    print(f\"Тип PyTorch тензора: {type(movie_vec_torch)}, Форма: {movie_vec_torch.shape}\")\n",
        "else:\n",
        "    print(f\"Слово '{word}' не найдено в модели (хотя FastText обычно генерирует для OOV).\")\n",
        "\n",
        "# Пример для OOV слова\n",
        "oov_word = \"filmography\" # Предположим, что этого слова нет в train_data.txt\n",
        "oov_vector = word_model.get_word_vector(oov_word)\n",
        "print(f\"Вектор для OOV слова '{oov_word}' (первые 5 элементов): {oov_vector[:5]}\")\n",
        "print(f\"Форма OOV вектора: {oov_vector.shape}\")\n",
        "\n",
        "\n",
        "# 2. Обучение модели классификации текста\n",
        "print(\"\\n2. Обучение модели классификации текста:\")\n",
        "# input: текстовый файл с метками\n",
        "# dim: размерность векторов\n",
        "# epoch: количество эпох обучения\n",
        "# lr: скорость обучения\n",
        "# autotuneValidationFile: файл для автонастройки параметров на основе валидационной выборки\n",
        "classifier_model = fasttext.train_supervised(train_data_file, dim=100, epoch=25, lr=0.1)\n",
        "\n",
        "# Предсказание класса для нового текста\n",
        "text_to_predict = \"I found it quite engaging.\"\n",
        "prediction = classifier_model.predict(text_to_predict)\n",
        "print(f\"Текст: '{text_to_predict}'\")\n",
        "print(f\"Предсказанная метка и вероятность: {prediction}\")\n",
        "\n",
        "# Предсказание нескольких меток (top N)\n",
        "prediction_top_k = classifier_model.predict(text_to_predict, k=2)\n",
        "print(f\"Предсказания (top 2): {prediction_top_k}\")\n",
        "\n",
        "# Оценка модели\n",
        "# В реальном сценарии вы бы использовали отдельный тестовый файл.\n",
        "print(\"\\nОценка модели (на обучающих данных для примера):\")\n",
        "results = classifier_model.test(train_data_file)\n",
        "print(f\"Примеры: {results[0]}, Точность: {results[1]}, Полнота: {results[2]}\")\n",
        "\n",
        "# 3. Получение вектора для целого предложения (классификация)\n",
        "# FastText для классификации также агрегирует векторы слов для предложения\n",
        "print(\"\\n3. Получение вектора для предложения (из классификационной модели):\")\n",
        "sentence_vector = classifier_model.get_sentence_vector(text_to_predict)\n",
        "print(f\"Вектор предложения (первые 5 элементов): {sentence_vector[:5]}\")\n",
        "# Использование с NumPy\n",
        "sentence_vec_np = sentence_vector\n",
        "print(f\"Тип NumPy вектора: {type(sentence_vec_np)}, Форма: {sentence_vec_np.shape}\")\n",
        "# Использование с PyTorch\n",
        "sentence_vec_torch = torch.from_numpy(sentence_vec_np)\n",
        "print(f\"Тип PyTorch тензора: {type(sentence_vec_torch)}, Форма: {sentence_vec_torch.shape}\")\n",
        "\n",
        "# Очистка созданного файла\n",
        "os.remove(train_data_file)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vTkbVnwk7OoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Объяснение: Что это делает?\n",
        "\n",
        "Каждая из этих библиотек решает различные аспекты обработки естественного языка, но все они в конечном итоге стремятся помочь машинам \"понять\" человеческий язык.\n",
        "\n",
        "  * **SpaCy:** Думайте о SpaCy как о высокоскоростном конвейере для NLP. Он берет необработанный текст и быстро преобразует его в структурированные данные (токены, сущности, части речи, зависимости), которые легко использовать для дальнейшего анализа или встраивать в приложения. Его встроенные модели уже \"знают\" о языке много вещей, таких как грамматика и синтаксис.\n",
        "  * **NLTK:** NLTK - это скорее \"инструментарий\" для NLP. Он предоставляет множество алгоритмов и корпусов, позволяя вам строить свои собственные NLP-системы с нуля, исследовать различные подходы и учиться. Он очень гибок, но требует больше ручной работы для создания производственных систем.\n",
        "  * **Gensim:** Gensim специализируется на \"смысле\" и \"темах\" в больших объемах текста. Он позволяет вам найти скрытые темы в статьях (как LDA), или получить числовые \"векторы\" слов, которые отражают их значение (как Word2Vec/FastText). Эти векторы очень важны, потому что они позволяют компьютерам выполнять математические операции со словами, что приводит к пониманию сходства и отношений между ними.\n",
        "  * **FastText (отдельная библиотека):** Это специализированный инструмент для создания очень эффективных векторов слов и классификаторов текста. Его уникальность в том, что он смотрит на слова не только как на целые единицы, но и как на комбинации их частей (символьных N-грамм). Это делает его особенно хорошим для языков с богатой морфологией (где у слов много окончаний и приставок) и для обработки слов, которые редко встречаются в обучающих данных.\n",
        "\n",
        "### Как NumPy и PyTorch используются с этими библиотеками?\n",
        "\n",
        "Основная идея заключается в том, что NLP-библиотеки помогают преобразовать текст в **числовые представления**. Эти числовые представления могут быть:\n",
        "\n",
        "  * **Разреженными векторами:** Например, в Bag-of-Words (как в Gensim), где каждое число показывает количество вхождений слова. Эти векторы часто имеют много нулей.\n",
        "  * **Плотными векторами (Word Embeddings/Word Vectors):** Это многомерные векторы (обычно 50-300 измерений), где каждое число не имеет прямого интерпретируемого значения, но *вместе* они кодируют семантику слова. Слова с похожим значением будут иметь векторы, которые находятся \"близко\" друг к другу в этом многомерном пространстве.\n",
        "\n",
        "**NumPy** используется для:\n",
        "\n",
        "  * **Обработки и анализа числовых данных:** После того как NLP-библиотека выдает числовые векторы (например, векторы слов из SpaCy, Gensim или FastText), NumPy идеально подходит для их дальнейшей манипуляции. Вы можете выполнять операции, такие как сложение векторов, вычисление косинусного сходства, усреднение векторов для создания вектора предложения, или построение матриц для машинного обучения.\n",
        "  * **Подготовка данных для PyTorch:** NumPy часто является промежуточным шагом, где данные подготавливаются перед преобразованием в тензоры PyTorch.\n",
        "\n",
        "**PyTorch** используется для:\n",
        "\n",
        "  * **Глубокого обучения (Deep Learning):** После того как текст преобразован в числовые тензоры (например, с помощью векторов слов), PyTorch используется для построения нейронных сетей. Вы можете подавать эти тензоры на вход слоям нейронной сети для задач, таких как:\n",
        "      * **Классификация текста:** Определение тональности, спама, категории документа.\n",
        "      * **Машинный перевод:** Преобразование текста из одного языка в другой.\n",
        "      * **Генерация текста:** Создание нового текста.\n",
        "      * **Ответы на вопросы:** Поиск ответов в тексте.\n",
        "  * **Автоматическое дифференцирование:** PyTorch позволяет автоматически вычислять градиенты, что критически важно для обучения нейронных сетей.\n",
        "  * **Оптимизация на GPU:** PyTorch эффективно использует графические процессоры (GPU) для ускорения вычислений, что очень важно для больших моделей и объемов данных.\n",
        "\n",
        "**В итоге:**\n",
        "\n",
        "  * **SpaCy, NLTK, Gensim, FastText (библиотека):** Преобразуют сырой текст в структурированные и/или числовые данные (токены, сущности, векторы слов).\n",
        "  * **NumPy:** Обрабатывает и манипулирует этими числовыми данными, часто выступая как мост к библиотекам глубокого обучения.\n",
        "  * **PyTorch:** Использует эти числовые данные (в виде тензоров) для построения и обучения сложных моделей глубокого обучения, которые могут \"понимать\" и генерировать язык.\n",
        "\n",
        "Надеюсь, этот подробный гайд поможет вам начать работу с этими мощными NLP-библиотеками\\!"
      ],
      "metadata": {
        "id": "tP8DsOuV7OoD"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}