В sklearn.model_selection есть несколько типов Fold-методов для cross-validation, каждый подходит для разных сценариев в Kaggle соревнованиях.

## Основные методы разбиения

**KFold** - базовый метод, делит данные на k равных частей случайным образом. Используется когда:
- Нет временной зависимости в данных
- Классы примерно сбалансированы
- Данные независимы и одинаково распределены

```python
from sklearn.model_selection import KFold
kf = KFold(n_splits=5, shuffle=True, random_state=42)
```

**StratifiedKFold** - сохраняет пропорции классов в каждом фолде. Критически важен для:
- Классификации с несбалансированными классами
- Когда нужно обеспечить представительность каждого класса в train/val
- Большинство Kaggle соревнований по классификации

```python
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
```

**GroupKFold** - не разделяет связанные объекты между фолдами. Используется когда:
- Есть группы связанных наблюдений (один пользователь, одна компания)
- Важно избежать data leakage между train/val
- Примеры: медицинские данные (пациенты), финансовые (компании)

```python
from sklearn.model_selection import GroupKFold
gkf = GroupKFold(n_splits=5)
# groups - массив с идентификаторами групп
```

## Специализированные методы

**TimeSeriesSplit** - для временных рядов, где будущие данные не должны попадать в обучение:
- Финансовые предсказания
- Прогнозирование продаж
- Любые данные с временной компонентой

```python
from sklearn.model_selection import TimeSeriesSplit
tss = TimeSeriesSplit(n_splits=5)
```

**StratifiedGroupKFold** - комбинирует стратификацию и группировку:
- Медицинская диагностика (группы пациентов + баланс диагнозов)
- Мультиклассовые задачи с группами

## Практические советы для Kaggle

1. **Всегда проверяйте описание данных** - ищите упоминания групп, временных зависимостей, иерархической структуры

2. **Мониторьте корреляцию между CV и LB scores** - если они сильно расходятся, возможно неправильно выбран метод разбиения

3. **Для временных данных** обязательно используйте временное разбиение, даже если это не указано явно

4. **При наличии групп** (пользователи, магазины, регионы) используйте GroupKFold для избежания переобучения

5. **Стратификация почти всегда полезна** в классификации, особенно с небольшими датасетами

6. **Количество фолдов**: обычно 5-10, но для маленьких датасетов можно больше

Правильный выбор CV стратегии часто является ключом к успеху в Kaggle - неподходящая валидация может привести к переобучению и плохим результатам на private leaderboard.
