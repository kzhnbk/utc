{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# End-to-End Machine Learning Pipeline with ChatGPT: A Beginner's Guide - part 3","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\nThis is part 3 of the **How to use ChatGPT in a competition** series, in which I show how to leverage the power of ChatGPT to build a complete pipeline without the need for extensive coding skills.\n\nIf you haven't already, first check out [part 1](https://www.kaggle.com/code/jacoporepossi/how-to-use-chatgpt-in-a-competition-eda-part-1/notebook) and [part 2](https://www.kaggle.com/code/jacoporepossi/how-to-use-chatgpt-in-a-competition-model-part-2), which cover EDA and baseline creation respectively.\n\nIn this notebook we are going to improve our previous baseline thanks to **hyperparameter tuning and ensembling using ChatGPT**!","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter tuning and ensembling with ChatGPT","metadata":{"tags":[]}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nimport pandas as pd\nimport numpy as np\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(\"ignore\", category=ConvergenceWarning)\nnp.random.seed(42)\nrandom.seed(42)\n\n\n# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/playground-series-s3e3/train.csv\")\n\n# Splitting the data into features and target\nX = data.drop(['Attrition', 'id'], axis=1)\ny = data['Attrition']\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n\n# Selecting the categorical features\ncategorical_features = X_train.select_dtypes(include='object').columns\n\n# Encoding the categorical features\nencoder = OneHotEncoder(sparse=False)\nX_train_cat = encoder.fit_transform(X_train[categorical_features])\nX_test_cat = encoder.transform(X_test[categorical_features])\n\n# Dropping the original categorical features\nX_train = X_train.drop(categorical_features, axis=1)\nX_test = X_test.drop(categorical_features, axis=1)\n\n\n# Concatenating the encoded categorical features\nX_train = pd.concat([X_train, pd.DataFrame(X_train_cat, index=X_train.index)], axis=1)\nX_test = pd.concat([X_test, pd.DataFrame(X_test_cat, index=X_test.index)], axis=1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-01-23T14:09:01.676600Z","iopub.execute_input":"2023-01-23T14:09:01.677167Z","iopub.status.idle":"2023-01-23T14:09:03.128490Z","shell.execute_reply.started":"2023-01-23T14:09:01.677064Z","shell.execute_reply":"2023-01-23T14:09:03.126889Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"In [part 2](https://www.kaggle.com/code/jacoporepossi/how-to-use-chatgpt-in-a-competition-model-part-2) **we didn't quite get a great roc_auc score**, in fact the purpose of the notebook was only to **test ChatGPT in creating a baseline** starting from a textual input.\n\nWill ChatGPT be able to provide better code to increase the performance obtained previously? Let's try! \n\n**Starting where we left off**, perhaps it makes sense to **provide feedback** that the performance hasn't been that great. Let's see **without additional help** what the algorithm proposes as possibile solution:\n\n<details><summary><i><mark style='background-color:burlywood'>PROMPT</mark> - the machine learning proposed didn't <b>achieved a decent roc_auc score</b>. Provide some <b>code to improve</b> the performance and <b>test the final optimized model</b> on the test set as before.</i></summary>\n<pre><code>\nThere are several ways to improve the performance of a machine learning model, some of which include:\n- Feature Engineering: You can create new features or transform existing features to improve the model's performance. For example, you could create interaction terms between features, or take the logarithm of a feature to reduce the impact of outliers.\n- Hyperparameter Tuning: You can use techniques like grid search or random search to find the best hyperparameters for your model. This can improve the model's performance by adjusting the learning rate, regularization strength, and other parameters.\n- Ensemble Methods: You can use ensemble methods like bagging, boosting, and stacking to improve the performance of your model. These methods combine the predictions of multiple models to create a more accurate final prediction.\n- Model selection: Try different models and compare their performance. For example, you can try Decision Trees, Random Forest, SVM, XGBoost, etc.\n- Cross-Validation: Use cross-validation techniques like K-Fold cross-validation to get a more robust evaluation of the model's performance. This can help you avoid overfitting and select a model that generalizes well to unseen data.\n\nHere's an example of how you can use GridSearchCV and RandomizedSearchCV to tune the hyperparameters of a logistic regression model:\n</code></pre>\n<br>\n<img src=\"https://i.imgur.com/vOjVKFz.png\">\n</details>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\n\n# Define the parameter grid\nparam_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}\n\n# Initialize the logistic regression model\nclf = LogisticRegression()\n\n# Initialize the GridSearchCV object\ngrid_search = GridSearchCV(clf, param_grid, cv=5)\n\n# Fit the GridSearchCV object to the data\ngrid_search.fit(X_train, y_train)\n\n# Get the best estimator\nbest_clf = grid_search.best_estimator_\n\n# Print the best parameters and the best score\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\n\n# Predict on the test set\ny_pred = best_clf.predict(X_test)\n\n# Evaluate the model\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC: {auc}')","metadata":{"execution":{"iopub.status.busy":"2023-01-23T14:09:03.131067Z","iopub.execute_input":"2023-01-23T14:09:03.132516Z","iopub.status.idle":"2023-01-23T14:09:05.905295Z","shell.execute_reply.started":"2023-01-23T14:09:03.132456Z","shell.execute_reply":"2023-01-23T14:09:05.904029Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"{'C': 0.1, 'penalty': 'l2'}\n0.8799395217222438\nAUC: 0.5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It's definitely interesting to read ChatGPT's \"opinions\", however the **more we delve into certain details** of machine learning, the more some information tends to be **quite vague**. For instance, in a bullet list of possibile ways to improve a model's performance, **cross validation, model selection and hyperparameters tuning should be somehow related to each other** and not presented as separated things/steps. This could lead to **misunderstandings** if used by an inexperienced Citizen Data Scientist.\n\nGiven that the performance didn't improve, let's further challange ChatGPT.\n\n<details><summary><i><mark style='background-color:burlywood'>PROMPT</mark> - we are still getting a low AUC score, <b>change the algorithm and optimize its parameters</b>. Provide only the code</i></summary>\n\n<br>\n<img src=\"https://i.imgur.com/sGLiYw3.png\">\n</details>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score\n\n# Define the parameter grid\nparam_grid = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, 20]}\n\n# Initialize the RandomForestClassifier\nclf = RandomForestClassifier()\n\n# Initialize the RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(clf, param_grid, cv=5)\n\n# Fit the RandomizedSearchCV object to the data\nrandom_search.fit(X_train, y_train)\n\n# Print the best parameters and the best score\nprint(random_search.best_params_)\nprint(random_search.best_score_)\n\n# Get the best estimator\nbest_clf = random_search.best_estimator_\n\n# Predict on the test set\ny_pred = best_clf.predict(X_test)\n\n# Evaluate the model\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC: {auc}')","metadata":{"execution":{"iopub.status.busy":"2023-01-23T14:09:05.907073Z","iopub.execute_input":"2023-01-23T14:09:05.907799Z","iopub.status.idle":"2023-01-23T14:09:20.469803Z","shell.execute_reply.started":"2023-01-23T14:09:05.907752Z","shell.execute_reply":"2023-01-23T14:09:20.468454Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"{'n_estimators': 50, 'max_depth': 10}\n0.8888919713699162\nAUC: 0.5324324324324324\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The code changed the algorithm to `RandomForestClassifier` and used `RandomizedSearchCV` to find the best hyperparameters. Indeed we were able to **increase our roc_auc score on the test set!**\nChatGPT was able to guess that `RandomForestClassifier` is generally more powerful compared to `LogistiRegression` when it comes to work with tabular data.\n\nCan we push a little bit further asking for an **ensemble of algorithms**?\n\n<details><summary><i><mark style='background-color:burlywood'>PROMPT</mark> - how can I <b>ensemble different models</b>, weighting them properly in order to increase my performance? Provide code and skip the explanation</i></summary>\n\n<br>\n<img src=\"https://i.imgur.com/AqgRX6p.png\">\n</details>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\n# Initialize the classifiers\nclf1 = LogisticRegression()\nclf2 = DecisionTreeClassifier()\nclf3 = RandomForestClassifier()\n\n# Create the ensemble\nensemble = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('rf', clf3)], voting='soft', weights=[1,1,1])\n\n# Fit the ensemble to the data\nensemble.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = ensemble.predict(X_test)\n\n# Evaluate the ensemble\nauc = roc_auc_score(y_test, y_pred)\nprint(f'AUC: {auc}')","metadata":{"execution":{"iopub.status.busy":"2023-01-23T14:09:20.472414Z","iopub.execute_input":"2023-01-23T14:09:20.473730Z","iopub.status.idle":"2023-01-23T14:09:21.022892Z","shell.execute_reply.started":"2023-01-23T14:09:20.473669Z","shell.execute_reply":"2023-01-23T14:09:21.020942Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"AUC: 0.5415540540540541\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Once again, we improved our score but <b>this is pure luck</b>, as we have no idea if the test set we set aside is representative of the entire problem.<br> Let's tell that to ChatGPT.\n\n<details><summary><i><mark style='background-color:burlywood'>PROMPT</mark> - the performance improved but probably is due to luck. Can we use <b>cross validation</b> to properly assess that the roc_auc is better? Provide a new code and skip the explanation.</i></summary>\n\n<br>\n<img src=\"https://i.imgur.com/r1tty9I.png\">\n</details>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Initialize the classifiers\nclf1 = LogisticRegression()\nclf2 = DecisionTreeClassifier()\nclf3 = RandomForestClassifier()\n\n# Create the ensemble\nensemble = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('rf', clf3)], voting='soft', weights=[1,1,1])\n\n# Use cross_val_score to assess the performance of the ensemble\nscores = cross_val_score(ensemble, X, y, cv=5, scoring='roc_auc')\nprint(scores)\n\n# Print the mean and standard deviation of the scores\nprint(f'Mean AUC: {scores.mean():.3f} (+/- {scores.std():.3f})')\n","metadata":{"execution":{"iopub.status.busy":"2023-01-23T14:09:21.025664Z","iopub.execute_input":"2023-01-23T14:09:21.026913Z","iopub.status.idle":"2023-01-23T14:09:21.113243Z","shell.execute_reply.started":"2023-01-23T14:09:21.026826Z","shell.execute_reply":"2023-01-23T14:09:21.111923Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[nan nan nan nan nan]\nMean AUC: nan (+/- nan)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We are getting all `nan` because we are using `X` and the **categorical features are not handled**.\n\n<details><summary><i><mark style='background-color:burlywood'>PROMPT</mark> - I get only nan results in the cross_val_score. This is because <b>X has not been preprocessed</b>. Fix the code considering all the information I gave to you in the previous prompts</i></summary>\n\n<br>\n<img src=\"https://i.imgur.com/ZnKl90q.png\">\n</details>\n<br>\n\nNope...we are starting all over again. Moreover this is not exactly what we want. I was expecting a code able to **handle the categorical encoding inside the cross_val_score**, and why not, also **hyperparameter tuning with GridSearchCV**. \n\nHere's my code to achieve that:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport pprint\n\n# Initialize the classifiers\nclf1 = LogisticRegression()\nclf2 = DecisionTreeClassifier()\nclf3 = RandomForestClassifier()\n\n# Init \nnumeric_features = X.select_dtypes(include='number').columns\nnumeric_transformer = StandardScaler()\n\ncategorical_features = X.select_dtypes(include='object').columns\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features),\n        (\"cat\", categorical_transformer, categorical_features),\n    ]\n)\n\n# Create the ensemble\nensemble = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('rf', clf3)], voting='soft', weights=[1,1,1])\n\n# Define the parameters for the GridSearchCV\nparam_grid = {\n    'dt__max_depth': [5, 10, 20],\n    'weights': [[1,1,1], [2,1,1], [1,2,1], [1,1,2]]\n}\ngrid = GridSearchCV(ensemble, param_grid, cv=3, verbose=1)\n\n# Put everything all together in a Pipeline\nclf = Pipeline(\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", grid)]\n)\n\n# Use cross_val_score to assess the performance of the ensemble\nscores = cross_validate(clf, X, y, cv=5, scoring='roc_auc', return_estimator=True)\nprint('\\nScores on the test folds:', scores['test_score'])\n\n# Print the mean and standard deviation of the scores\nprint('Mean AUC: {:.3f} (+/- {:.3f})'.format(scores['test_score'].mean(), scores['test_score'].std()))\n\n# Best params found for each fold\nprint('Best estimators found:')\npprint.pprint([estim.steps[1][1].best_params_ for estim in scores['estimator']])","metadata":{"execution":{"iopub.status.busy":"2023-01-23T14:09:21.114875Z","iopub.execute_input":"2023-01-23T14:09:21.115381Z","iopub.status.idle":"2023-01-23T14:10:42.998249Z","shell.execute_reply.started":"2023-01-23T14:09:21.115328Z","shell.execute_reply":"2023-01-23T14:10:42.996497Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 12 candidates, totalling 36 fits\nFitting 3 folds for each of 12 candidates, totalling 36 fits\nFitting 3 folds for each of 12 candidates, totalling 36 fits\nFitting 3 folds for each of 12 candidates, totalling 36 fits\nFitting 3 folds for each of 12 candidates, totalling 36 fits\n\nScores on the test folds: [0.78327703 0.85346284 0.76389831 0.81762712 0.8490678 ]\nMean AUC: 0.813 (+/- 0.035)\nBest estimators found:\n[{'dt__max_depth': 10, 'weights': [2, 1, 1]},\n {'dt__max_depth': 5, 'weights': [1, 1, 2]},\n {'dt__max_depth': 10, 'weights': [2, 1, 1]},\n {'dt__max_depth': 5, 'weights': [2, 1, 1]},\n {'dt__max_depth': 10, 'weights': [1, 1, 2]}]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this code we are performing **cross validation** using `cross_validate`, which is able to **return also the estimator** used on the test fold. **Within the cross validation**, we are **processing** the categorical features and the numerical ones with `ColumnTransformer`, and performing **hyperparameter tuning** with `GridSearchCV`.\nThis resembles the framework [I shared about nested cross validation](https://www.kaggle.com/code/jacoporepossi/nested-cross-validation-and-ensembling-framework).\n\nWe can see that the **performace are quite different** than those obtained before with ChatGPT, mainly because:\n- I **fine-tuned our VotingClassifier estimators and the weights associated to it**\n- `cross_validate` uses `predict_proba` and not `predict`, which will **yield higher roc_auc score**. The metric uses the raw probabilities to calculate the best threshold to differentiate the two classes, whereas if we use `predict` we are using the **default threshold 0.5**","metadata":{}},{"cell_type":"markdown","source":"# Submission\n\nIt's time to submit our predictions to Kaggle. How can we do that using our VotingClassifier?\n\nIn what I think is the [best answer about nested cross validation and model selection](https://stats.stackexchange.com/questions/65128/nested-cross-validation-for-model-selection), I've learned that we can select **any estimator** we previously found in a cross validation scheme as long as **our performance are quite stable**. Stability means that the **model does not change (much)** if the training data is \"perturbed\" by replacing a few cases by other cases.<br>\nGiven that the **standard deviation** is `0.04`, we can reasonably say that **any model** we created so far would yield a score around `0.8 +- 0.04`, so let's just **select the first estimator** fitted and let's make a submission.\n\nDue to the fact that the **estimator is in a pipeline**, we are able to predict knowing that the **preprocessing will be handled automatically by sklearn!**","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/playground-series-s3e3/sample_submission.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s3e3/test.csv').drop(['id'], axis=1)\n\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-23T14:10:43.006913Z","iopub.execute_input":"2023-01-23T14:10:43.012829Z","iopub.status.idle":"2023-01-23T14:10:43.079073Z","shell.execute_reply.started":"2023-01-23T14:10:43.012731Z","shell.execute_reply":"2023-01-23T14:10:43.078102Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"     id  Attrition\n0  1677   0.119261\n1  1678   0.119261\n2  1679   0.119261\n3  1680   0.119261\n4  1681   0.119261","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Attrition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1677</td>\n      <td>0.119261</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1678</td>\n      <td>0.119261</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1679</td>\n      <td>0.119261</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1680</td>\n      <td>0.119261</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1681</td>\n      <td>0.119261</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"preds = scores['estimator'][0].predict_proba(test)[:, 1]\npreds","metadata":{"execution":{"iopub.status.busy":"2023-01-23T14:10:43.080677Z","iopub.execute_input":"2023-01-23T14:10:43.081829Z","iopub.status.idle":"2023-01-23T14:10:43.164487Z","shell.execute_reply.started":"2023-01-23T14:10:43.081776Z","shell.execute_reply":"2023-01-23T14:10:43.163046Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([0.12869493, 0.11766861, 0.0242274 , ..., 0.04729937, 0.0514997 ,\n       0.01396995])"},"metadata":{}}]},{"cell_type":"code","source":"submission['Attrition'] = preds\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-23T14:10:43.166320Z","iopub.execute_input":"2023-01-23T14:10:43.167495Z","iopub.status.idle":"2023-01-23T14:10:43.188106Z","shell.execute_reply.started":"2023-01-23T14:10:43.167433Z","shell.execute_reply":"2023-01-23T14:10:43.186320Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\nIn this final notebook of the **How to use ChatGPT in a competition** series, we focused on improving the performance of our machine learning model and submitting it to Kaggle.\n\nThroughout the notebook, we saw how ChatGPT could provide **some basic guidance** but **we needed to correct it quite a few times** as it was leading us to a **wrong implementation**.\n\nI think this highlights the **importance of having a good understanding** of the machine learning workflow and the specific task at hand when using ChatGPT. For a beginner, **ChatGPT could lead to a disaster**, as it may provide incorrect information just to **\"please\" the user**. \n\nLet me give you a final example to convince you about that:\n\n<img src=\"https://i.imgur.com/rGAH2Rt.png\" width=700>\n\nIt's clear that **based on how you ask the question**, ChatGPT will try to provide the best answer in what **I think is a deductive way**: if you are asking why something is happening, it probably **means that it should be like that**. It's like being asked why 2+2=4 at school...you probably wouldn't argue with the teacher that 2+2 is not 4, but rather try to come up with the best answer to demonstrate that.<br>\n**That's exactly why might get fooled by ChatGPT: it plays to our expectations. It tries to predict what we foresee and what seems to be the general opinion held by experts in some domains.**\n\nHowever, the area where ChatGPT has shown quite remarkable results is Exploratory Data Analysis. EDA is often full of code tricks you should know, sometimes just to have some basic plots. Here we can **definitely leverage the tool to speed up the process** and collect some quick insights about the dataset.\n\nIn conclusion, these are my **3 basic rules** that I've learned about ChatGPT:\n 1. with **minumum effort prompts**, you'll get **low quality results**. It takes time and effor to fine-tune an answer\n 2. **don't trust anything it says** unless you check online if the answer makes sense\n 3. **don't ask confirmatory questions** unless you have already an idea of where you are going. A confirmatory question means a question that leads to answers that can only support a certain point. Between \"why do we do this instead of that?\", try \"should we do this or that?\" or \"what should we do?\"\n \nWho knows what the future will hold and how Data Scientist will evolve to adapt to new AI tool advances.\n\nIf you've made it this far, thank you for your interest and I hope it helped!","metadata":{}}]}