{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":100973,"databundleVersionId":12209210,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TASK DESCRIPTION","metadata":{"_uuid":"ad359b05-7b4d-462f-94b4-05e4d1378268","_cell_guid":"14d1b415-cc75-4942-a324-c81076924923","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"**Legend:**\n\nYoung Alex has a beloved BERT model that he carries everywhere on his trusty flash drive. One day, during an excursion along the River Styx, a few drops of water landed on the precious device, corrupting the model's weights.\n\nHeartbroken, Alex rushed home to fix the neural network. After quick analysis, he discovered only the token embeddings were damaged - the rest of the architecture (attention blocks and heads) remained perfectly intact. Now he needs to restore the model's performance on Sentiment Analysis Task.\n\n**Task:**\n\nYou need to fix the broken vectors of the Embeddings matrix of the model so as to improve the quality of the model on the task of text sentiment analysis.\n\n**Restrictions:**\n\n- You can not use any other transformer based pre-trained models and LLMs.\n\n- You can not any additional data\n\n- You can not fine-tune or pre-train model\n\n===\n\nWhen you make a submit, make a Quick Save of the notebook, otherwise we may reject your solution.\n\nYou must solve this task on KAGGLE (YOU CAN'T USE CLOUD.RU)\n\n==========\n\n**Легенда:**\n\nYoung Alex имеет любимую модель BERT, которую он везде носит на своей надежной флешке. Однажды, во время экскурсии вдоль реки Стикс, несколько капель воды попало на драгоценное устройство, повредив веса модели.\n\nС разбитым сердцем Алекс поспешил домой, чтобы починить нейронную сеть. После быстрого анализа он обнаружил, что повреждены только эмбеддинги токенов — остальная архитектура (блоки внимания и головы) осталась полностью нетронутой.\n\nТеперь ему нужно восстановить производительность модели, оставив все остальные веса замороженными (никакие изменения в механизмах внимания или других компонентах не допускаются). Ваша задача — помочь Алексу достичь этой цели, не нарушив его ностальгическую привязанность к оригинальной модели.\n\n**Задача:**\n\nВам необходимо починить сломанные вектора матрицы Embeddings модели так, чтобы улучшить качество модели на задаче анализа тональности текста.\n\n**Ограничения:**\n\n- Вы не можете использовать никакие другие предобученные модели на основе архитектуры Трансформер и LLM.\n\n- Вы не можете использовать никакие дополнительные данные.\n\n- Вы не можете дообучать или предобучать модель.\n\n===\n\nПри отправке решения сделайте Quick Save ноутбука, иначе мы можем отклонить ваше решение.\n\nЭту задачу необходимо решить на KAGGLE (ВЫ НЕ МОЖЕТЕ ИСПОЛЬЗОВАТЬ CLOUD.RU)","metadata":{"_uuid":"b6dad3d4-db79-4e10-bea1-2b5221c053b5","_cell_guid":"53a8c0c7-1d42-46f8-a9fc-38cec469e8a7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# DEPENDINGS","metadata":{"_uuid":"729e7b33-f336-4158-91b9-2ec42f07fb2c","_cell_guid":"2c240aad-5cd9-4512-b354-16638f24de91","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nnp.random.seed(21)","metadata":{"_uuid":"0e0495df-b2a1-4e9e-8601-1538d7e0b844","_cell_guid":"2b5ad110-ba24-4aa7-9218-f9ff34479585","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-12T09:39:33.861629Z","iopub.execute_input":"2025-06-12T09:39:33.861863Z","iopub.status.idle":"2025-06-12T09:39:39.911530Z","shell.execute_reply.started":"2025-06-12T09:39:33.861839Z","shell.execute_reply":"2025-06-12T09:39:39.910945Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LOAD DATASET","metadata":{"_uuid":"625e3ed0-e8de-4e5f-a5a6-20f76373d94b","_cell_guid":"836f93b7-2871-44e7-95fa-d1f702aad554","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"val_data_path = \"/kaggle/input/neoai-2025-broken-bert/val_dataset.csv\"\ntest_data_path = \"/kaggle/input/neoai-2025-broken-bert/test.csv\"\n\nval_df = pd.read_csv(val_data_path)\n\ntest_df = pd.read_csv(test_data_path)","metadata":{"_uuid":"b4a48ef6-0be2-4a7a-bdab-10668899c857","_cell_guid":"d332409b-1bf9-470a-8954-3c444c1d3b35","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-12T09:39:39.912933Z","iopub.execute_input":"2025-06-12T09:39:39.913318Z","iopub.status.idle":"2025-06-12T09:39:39.956147Z","shell.execute_reply.started":"2025-06-12T09:39:39.913298Z","shell.execute_reply":"2025-06-12T09:39:39.955577Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LOAD TOKENIZER & MODEL","metadata":{"_uuid":"862c1a1d-16fa-4713-980f-1d8d08a3cf65","_cell_guid":"137d38f4-e403-4284-b836-dc9cf1d79558","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Ilseyar-kfu/broken_bert\")","metadata":{"_uuid":"f4d1af4f-6273-4970-8fda-fadb80246427","_cell_guid":"17b82aeb-64da-4817-ae36-b4277e5f9436","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-12T09:41:12.352114Z","iopub.execute_input":"2025-06-12T09:41:12.352957Z","iopub.status.idle":"2025-06-12T09:41:17.492051Z","shell.execute_reply.started":"2025-06-12T09:41:12.352928Z","shell.execute_reply":"2025-06-12T09:41:17.491172Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"_uuid":"275dcf7f-4018-4d0f-9c05-405f556a679e","_cell_guid":"b897de9c-0575-4cd7-a606-7df238c36373","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-12T09:41:18.810229Z","iopub.execute_input":"2025-06-12T09:41:18.810951Z","iopub.status.idle":"2025-06-12T09:41:18.816084Z","shell.execute_reply.started":"2025-06-12T09:41:18.810925Z","shell.execute_reply":"2025-06-12T09:41:18.815305Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_encodings = tokenizer(val_df[\"text\"].to_list(), truncation=True, padding=True, max_length=256)\nval_dataset = Dataset(val_encodings, val_df[\"labels\"].to_list())","metadata":{"_uuid":"d8a52f61-6081-4f8b-8cf5-c85d17cd7687","_cell_guid":"ee39bbf5-1864-42dd-a2f8-9d68a602e8e4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-12T09:41:21.818644Z","iopub.execute_input":"2025-06-12T09:41:21.818927Z","iopub.status.idle":"2025-06-12T09:41:21.975844Z","shell.execute_reply.started":"2025-06-12T09:41:21.818902Z","shell.execute_reply":"2025-06-12T09:41:21.975173Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"texts_2_score = val_df[\"text\"].to_list() + test_df[\"text\"].to_list()","metadata":{"_uuid":"9a01a7c4-80df-4490-9ddb-856b149b8d1e","_cell_guid":"53fee988-a632-4f99-a239-81a8e1c9bcee","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-12T09:41:22.172631Z","iopub.execute_input":"2025-06-12T09:41:22.173355Z","iopub.status.idle":"2025-06-12T09:41:22.177350Z","shell.execute_reply.started":"2025-06-12T09:41:22.173329Z","shell.execute_reply":"2025-06-12T09:41:22.176725Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODEL CHANGES","metadata":{"_uuid":"c5bc4f06-55b2-471c-af3b-d64f6c69cb9b","_cell_guid":"8f65562d-540b-46d8-8e21-3a632f6a063e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\"Ilseyar-kfu/broken_bert\")\n\nnew_embedings = model.bert.embeddings.word_embeddings.weight.detach().numpy().copy()\n\n# There's magic going on here!!! And we get very new !!! new_embedings !!!\n\nmodel.bert.embeddings.word_embeddings.weight = torch.nn.Parameter(torch.Tensor(new_embedings))","metadata":{"_uuid":"ced309a2-7146-4d18-9a61-b4b35f588f03","_cell_guid":"2a2d3fe2-852a-4539-8788-581b5f7248ac","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-12T10:01:32.106603Z","iopub.execute_input":"2025-06-12T10:01:32.107240Z","iopub.status.idle":"2025-06-12T10:01:32.536273Z","shell.execute_reply.started":"2025-06-12T10:01:32.107214Z","shell.execute_reply":"2025-06-12T10:01:32.535742Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"new_embedings.shape","metadata":{"_uuid":"b45221f2-bd4e-4d81-bcb8-38cd630aab9d","_cell_guid":"3d96adbb-6e35-4c9c-9d9c-870be9ecac99","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-12T10:01:33.478605Z","iopub.execute_input":"2025-06-12T10:01:33.479338Z","iopub.status.idle":"2025-06-12T10:01:33.483959Z","shell.execute_reply.started":"2025-06-12T10:01:33.479312Z","shell.execute_reply":"2025-06-12T10:01:33.483351Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"(30522, 768)"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport torch\nimport numpy as np\n\n# Load the pre-trained model with corrupted embeddings\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Ilseyar-kfu/broken_bert\")\n\n# --- Start of the fix ---\n\n# 1. Extract the embedding matrix and its dimensions\nembeddings = model.bert.embeddings.word_embeddings.weight.detach().cpu().numpy()\nembedding_dim = embeddings.shape[1]\n\n# 2. Reset the [PAD] token's embedding to a zero vector for stability\npad_token_id = tokenizer.pad_token_id\nif pad_token_id is not None:\n    embeddings[pad_token_id] = np.zeros(embedding_dim)\n\n# 3. Center the embedding data by subtracting the mean\nmean_vector = np.mean(embeddings, axis=0)\ncentered_embeddings = embeddings - mean_vector\n\n# 4. Apply PCA to de-noise the embeddings.\n# We retain the components that explain 99% of the variance.\npca = PCA(n_components=0.99)\ntransformed_embeddings = pca.fit_transform(centered_embeddings)\n\n# 5. Reconstruct the embeddings from the reduced PCA space\nreconstructed_centered_embeddings = pca.inverse_transform(transformed_embeddings)\n\n# 6. Add the mean vector back to the reconstructed embeddings\nreconstructed_embeddings = reconstructed_centered_embeddings + mean_vector\n\n# The 'magic' is the de-noised embedding matrix\nnew_embedings = reconstructed_embeddings\n\n# --- End of the fix ---\n\n# 7. Load the repaired embeddings back into the model\nmodel.bert.embeddings.word_embeddings.weight = torch.nn.Parameter(torch.Tensor(new_embedings))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T10:21:04.513702Z","iopub.execute_input":"2025-06-12T10:21:04.514011Z","iopub.status.idle":"2025-06-12T10:21:06.860446Z","shell.execute_reply.started":"2025-06-12T10:21:04.513988Z","shell.execute_reply":"2025-06-12T10:21:06.859853Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"# %% [code]\nimport numpy as np\nimport torch\n\n# Загружаем модель\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Ilseyar-kfu/broken_bert\")\n\n# --- Начало нового метода исправления ---\n\n# 1. Извлекаем эмбеддинги\nembeddings = model.bert.embeddings.word_embeddings.weight.detach().cpu().numpy().copy()\n\n# 2. Для каждого вектора вычисляем его длину (L2-норму)\nnorms = np.linalg.norm(embeddings, axis=1)\n\n# 3. Находим \"выбросы\" по нормам. Используем перцентили для определения \"нормального\" диапазона.\n# Это более надежно, чем среднее и стандартное отклонение.\np_low = np.percentile(norms, 1)  # Нижняя граница нормы (1-й перцентиль)\np_high = np.percentile(norms, 99) # Верхняя граница нормы (99-й перцентиль)\n\n# Находим индексы векторов, которые выходят за пределы \"нормального\" диапазона\noutlier_indices = np.where((norms < p_low) | (norms > p_high))[0]\ngood_indices = np.where((norms >= p_low) & (norms <= p_high))[0]\n\nprint(f\"Найдено {len(outlier_indices)} предположительно поврежденных векторов.\")\n\n# 4. Вычисляем средний вектор на основе всех \"хороших\" эмбеддингов\nif len(good_indices) > 0:\n    # Вычисляем среднее значение только по \"хорошим\" векторам\n    mean_good_embedding = np.mean(embeddings[good_indices], axis=0)\n\n    # 5. Заменяем все аномальные векторы на вычисленный средний вектор\n    if len(outlier_indices) > 0:\n        embeddings[outlier_indices] = mean_good_embedding\n        print(\"Поврежденные векторы заменены на среднее значение 'хороших' векторов.\")\nelse:\n    print(\"Не удалось найти 'хорошие' векторы. Эмбеддинги не изменены.\")\n\n# Наша \"магия\" — это исправленная матрица эмбеддингов\nnew_embedings = embeddings\n\n# --- Конец нового метода исправления ---\n\n# Загружаем исправленные эмбеддинги обратно в модель\nmodel.bert.embeddings.word_embeddings.weight = torch.nn.Parameter(torch.Tensor(new_embedings))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T10:25:34.872839Z","iopub.execute_input":"2025-06-12T10:25:34.873106Z","iopub.status.idle":"2025-06-12T10:25:35.279773Z","shell.execute_reply.started":"2025-06-12T10:25:34.873090Z","shell.execute_reply":"2025-06-12T10:25:35.278968Z"}},"outputs":[{"name":"stdout","text":"Найдено 306 предположительно поврежденных векторов.\nПоврежденные векторы заменены на среднее значение 'хороших' векторов.\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"# %% [markdown]\n# # FIX EMBEDDINGS VIA WORD2VEC TRAINING ON PROVIDED TEXTS\n# Train non-transformer Word2Vec embeddings on the val+test corpus,\n# then map them into the BERT embedding matrix.\n\n# %% [code]\nfrom gensim.models import Word2Vec\nfrom transformers import AutoModelForSequenceClassification\nimport numpy as np\nimport torch\n\n# collect token-level corpus using BERT tokenizer\ncorpus = [tokenizer.tokenize(text) for text in texts_2_score]\n\n# train Word2Vec with small vector size matching BERT hidden size\nwv_size = model.config.hidden_size  # typically 768\nw2v = Word2Vec(corpus, vector_size=wv_size, window=5, min_count=1, workers=4, epochs=10)\n\n# build new embedding matrix\nvocab = tokenizer.get_vocab()\nvocab_size = model.config.vocab_size\n\new_matrix = np.zeros((vocab_size, wv_size), dtype=np.float32)\navg_vec = np.mean(w2v.wv.vectors, axis=0)\n\nfor token, idx in vocab.items():\n    if token in w2v.wv:\n        ew = w2v.wv[token]\n    else:\n        ew = avg_vec\n    ew_matrix[idx] = ew\n\n# assign to model\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Ilseyar-kfu/broken_bert\")\nmodel.bert.embeddings.word_embeddings.weight = torch.nn.Parameter(torch.tensor(ew_matrix))\n\n# freeze everything except embeddings (optional)\nfor name, param in model.named_parameters():\n    if 'embeddings.word_embeddings' not in name:\n        param.requires_grad = False\n\n# %% [markdown]\n# Now evaluate on validation set as before:\n# %% [code]\nevaluate_on_validation(model, tokenizer, val_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EVALUATION","metadata":{"_uuid":"fd50d654-59d0-499b-bccc-5efc4f3de48e","_cell_guid":"18b79930-ad6e-4d18-9195-76963853cbf2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom numpy import argmax\nfrom transformers import pipeline\nimport wandb\nwandb.init(mode= \"disabled\")","metadata":{"_uuid":"7e25311d-7436-47db-9f33-577745213abe","_cell_guid":"497a7a03-f007-4014-91ec-4d66e97d1b6e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-12T10:26:53.130717Z","iopub.execute_input":"2025-06-12T10:26:53.131372Z","iopub.status.idle":"2025-06-12T10:26:53.143926Z","shell.execute_reply.started":"2025-06-12T10:26:53.131348Z","shell.execute_reply":"2025-06-12T10:26:53.143081Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/bam9f248?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x78f9f13fe5d0>"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ndef evaluate_on_validation(model, tokenizer, df_val):\n    label_2_dict = {'LABEL_0': 'neutral', \"LABEL_1\" : 'positive', \"LABEL_2\": 'negative'}\n    classifier = pipeline(\"text-classification\", model= model, tokenizer = tokenizer)\n    answ = classifier.predict(list(df_val[\"text\"]))\n    answ = [label_2_dict[el[\"label\"]] for el in answ]\n    \n    # print(f1_score(p.label_ids, preds, average='macro'))\n    print(classification_report(df_val[\"labels\"], answ))","metadata":{"_uuid":"cb729d2f-68de-4960-af1f-e17f71af1dcc","_cell_guid":"11b7a28c-a1f9-4c5f-81e4-7f58780279dd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-12T10:26:53.646385Z","iopub.execute_input":"2025-06-12T10:26:53.646966Z","iopub.status.idle":"2025-06-12T10:26:53.651362Z","shell.execute_reply.started":"2025-06-12T10:26:53.646945Z","shell.execute_reply":"2025-06-12T10:26:53.650538Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"evaluate_on_validation(model, tokenizer, val_df)","metadata":{"_uuid":"ec16da98-38b7-4e02-8a09-fc4b9d257562","_cell_guid":"732d1e73-5f39-4d09-9967-4cb81ca9c8cd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-12T10:25:40.081311Z","iopub.execute_input":"2025-06-12T10:25:40.081888Z","iopub.status.idle":"2025-06-12T10:25:59.296492Z","shell.execute_reply.started":"2025-06-12T10:25:40.081866Z","shell.execute_reply":"2025-06-12T10:25:59.295836Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n    negative       0.60      0.18      0.27       935\n     neutral       0.32      0.91      0.47       759\n    positive       0.62      0.06      0.11       806\n\n    accuracy                           0.36      2500\n   macro avg       0.52      0.38      0.29      2500\nweighted avg       0.52      0.36      0.28      2500\n\n","output_type":"stream"}],"execution_count":87},{"cell_type":"markdown","source":"# MODEL SCORING\nWhen you make a submit, \n1. Make a Quick Save of the notebook, otherwise we may reject your solution! \n2. Add notebook version to the comment for the submit.\n\n===\n\nПри отправке решения:\n\n1. Сделайте Quick Save ноутбука, иначе мы можем отклонить ваше решение!\n2. Добавьте версию ноутбука в комментарий к отправке.","metadata":{"_uuid":"bc2b32bf-1de4-41e3-9fdc-10b2fd31a186","_cell_guid":"fb0ae812-c402-41f7-90e1-22a01904f3d8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import hashlib\n\ndef create_submission(model, tokenizer, df_test):\n    label_2_dict = {'LABEL_0': 'neutral', \"LABEL_1\" : 'positive', \"LABEL_2\": 'negative'}\n    classifier = pipeline(\"text-classification\", model= model, tokenizer = tokenizer)\n    answ = classifier.predict(list(df_test[\"text\"]))\n    answ = [label_2_dict[el[\"label\"]] for el in answ]\n    \n    df = pd.DataFrame({\"labels\" : answ, \"id\": df_test['id']})\n    hsh = hashlib.sha256(df.to_csv(index=False).encode('utf-8')).hexdigest()[:8]\n    submit_path = f\"submit_{hsh}.csv\"\n    print(f\"SUBMIT_NAME: {submit_path}\")\n    print(df.head(10))\n    df.to_csv(submit_path,index=False)","metadata":{"_uuid":"3a9a0442-0fb3-4302-a99c-9882b7ae8d2b","_cell_guid":"078d977d-f9ce-4370-a9fc-6a9b49f6a35b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-12T10:21:43.129468Z","iopub.execute_input":"2025-06-12T10:21:43.130024Z","iopub.status.idle":"2025-06-12T10:21:43.135019Z","shell.execute_reply.started":"2025-06-12T10:21:43.130000Z","shell.execute_reply":"2025-06-12T10:21:43.134300Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"create_submission(model, tokenizer, test_df)","metadata":{"_uuid":"7301be6d-03a3-4877-8b8f-ca055747dbea","_cell_guid":"083cfeb2-408a-4e72-a313-093906148aef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-12T10:21:44.364817Z","iopub.execute_input":"2025-06-12T10:21:44.365473Z","iopub.status.idle":"2025-06-12T10:22:03.099391Z","shell.execute_reply.started":"2025-06-12T10:21:44.365442Z","shell.execute_reply":"2025-06-12T10:22:03.098705Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"SUBMIT_NAME: submit_e5690a38.csv\n     labels    id\n0   neutral  5000\n1   neutral  5001\n2   neutral  5002\n3   neutral  5003\n4   neutral  5004\n5   neutral  5005\n6   neutral  5006\n7  negative  5007\n8   neutral  5008\n9   neutral  5009\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}